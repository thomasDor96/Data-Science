{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Validation\n",
    "\n",
    "Model selection and validation are fundamental steps in statistical learning applications. In particular, we wish to select the model that performs optimally, both with respect to the training data and to external data. A model's performance on external data is known as its generalization performance. \n",
    "\n",
    "Depending on the type of learning method we use, we may be interested in one or more of the following:\n",
    "\n",
    "* how many variables should be included in the model?\n",
    "* what hyperparameter values should be used in fitting the model?\n",
    "* how many groups should we use to cluster our data?\n",
    "\n",
    "We will almost almost always use a model's generalization performance to answer these questions.\n",
    "\n",
    "[Givens and Hoeting (2012)](#references) includes a dataset for salmon spawning success. We can use this data to fix ideas. If we plot the number of recruits against the number of spawners, we see a distinct positive relationship, as we would expect. The question is, *what sort of polynomial relationship best describes the relationship?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "salmon = pd.read_table(\"../data/salmon.dat\", sep=r'\\s+', index_col=0)\n",
    "salmon.plot(x='spawners', y='recruits', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the one extreme, a linear relationship is underfit; on the other, we see that including a very large number of polynomial terms is clearly overfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "xvals = np.arange(salmon.spawners.min(), salmon.spawners.max())\n",
    "\n",
    "fit1 = np.polyfit(salmon.spawners, salmon.recruits, 1)\n",
    "p1 = np.poly1d(fit1)\n",
    "axes[0].plot(xvals, p1(xvals))\n",
    "axes[0].scatter(x=salmon.spawners, y=salmon.recruits)\n",
    "\n",
    "fit15 = np.polyfit(salmon.spawners, salmon.recruits, 15)\n",
    "p15 = np.poly1d(fit15)\n",
    "axes[1].plot(xvals, p15(xvals))\n",
    "axes[1].scatter(x=salmon.spawners, y=salmon.recruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select an appropriate polynomial order for the model using **cross-validation**, in which we hold out a testing subset from our dataset, fit the model to the remaining data, and evaluate its performance on the held-out subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(salmon.spawners, \n",
    "                            salmon.recruits, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural criterion to evaluate model performance is root mean square error.\n",
    "\n",
    "$$L\\left(Y, \\hat{f}(X) \\right) = \\sqrt{\\frac{1}{N}(Y - \\hat{f}(X))^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(x, y, coefs):\n",
    "    yfit = np.polyval(coefs, x)\n",
    "    return mean_squared_error(y, yfit) ** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the model at varying polynomial degrees, and compare their fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degrees = np.arange(11)\n",
    "train_err = np.zeros(len(degrees))\n",
    "validation_err = np.zeros(len(degrees))\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    p = np.polyfit(xtrain, ytrain, d)\n",
    "\n",
    "    train_err[i] = rmse(xtrain, ytrain, p)\n",
    "    validation_err[i] = rmse(xtest, ytest, p)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(degrees, validation_err, lw=2, label = 'cross-validation error')\n",
    "ax.plot(degrees, train_err, lw=2, label = 'training error')\n",
    "\n",
    "ax.legend(loc=0)\n",
    "ax.set_xlabel('degree of fit')\n",
    "ax.set_ylabel('rms error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cross-validation above, notice that the error is high for both very low and very high polynomial values, while training error declines monotonically with degree. The cross-validation error (sometimes referred to as test error or generalization error) is composed of two components: **bias** and **variance**. When a model is underfit, bias is low but variance is high, while when a model is overfit, the reverse is true.\n",
    "\n",
    "One can show that the MSE decomposes into a sum of the bias (squared) and variance of the estimator:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{Var}(\\hat{\\theta}) &= E[\\hat{\\theta} - \\theta]^2 - (E[\\hat{\\theta} - \\theta])^2 \\\\\n",
    "\\Rightarrow E[\\hat{\\theta} - \\theta]^2 &= \\text{Var}(\\hat{\\theta}) + \\text{Bias}(\\hat{\\theta})^2\n",
    "\\end{aligned}$$\n",
    "\n",
    "The training error, on the other hand, does not have this tradeoff; it will always decrease (or at least, never increase) as variables (polynomial terms) are added to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information-theoretic Model Selection\n",
    "\n",
    "One approach to model selection relies on the in-sample prediction error. One popular approach uses an information-theoretic criterion to identify the most appropriate model. Akaike (1973) found a formal relationship between Kullback-Leibler information (a dominant paradigm in information and coding theory) and likelihood theory. Akaike's Information Criterion (AIC) is an estimator of expected relative K-L information based on the maximized log-likelihood function, corrected for asymptotic bias. \n",
    "\n",
    "$$\\text{AIC} = -2 \\log(L(\\theta|data)) + 2K$$\n",
    "\n",
    "AIC balances the fit of the model (in terms of the likelihood) with the number of parameters required to achieve that fit. We can easily calculate AIC from the residual sums of squares as:\n",
    "\n",
    "$$\\text{AIC} = n \\log(\\text{RSS}/n) + 2k$$\n",
    "\n",
    "where $k$ is the number of parameters in the model. Notice that as the number of parameters increase, the residual sum of squares goes down, but the second term (a penalty) increases.\n",
    "\n",
    "To apply AIC to a model selection problem, we choose the model that has the lowest AIC value.\n",
    "\n",
    "[AIC can be shown to be equivalent to leave-one-out cross-validation](http://www.jstor.org/stable/2984877)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aic(rss, n, k):\n",
    "    return n * np.log(float(rss) / n) + 2 * k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use AIC to select the appropriate polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aic_values = np.zeros(len(degrees))\n",
    "params = np.zeros((len(degrees), len(degrees)))\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    p, residuals, rank, singular_values, rcond = np.polyfit(\n",
    "                                salmon.spawners, salmon.recruits, d, full=True)\n",
    "    aic_values[i] = aic((residuals).sum(), len(salmon.spawners), d+1)\n",
    "    params[i, :(d+1)] = p\n",
    "\n",
    "plt.plot(degrees, aic_values, lw=2)\n",
    "plt.xlabel('degree of fit')\n",
    "plt.ylabel('AIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of interpretation, AIC values can be transformed into model weights via:\n",
    "\n",
    "$$p_i = \\frac{\\exp^{-\\frac{1}{2} AIC_i}}{\\sum_j \\exp^{-\\frac{1}{2} AIC_j} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aic_trans = np.exp(-0.5 * aic_values)\n",
    "aic_probs = aic_trans / aic_trans.sum()\n",
    "aic_probs.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for Classification\n",
    "\n",
    "For classifiers such as decision trees and random forests, we may judge our model performance a little differently than the above.\n",
    "\n",
    "First, let's describe four concepts to help evaluate a classifier.\n",
    "\n",
    "A **true positive** (TP) occurs when we correctly predict the positive class.\n",
    "\n",
    "A **true negative** (TN) occurs when we correctly predict the negative class.\n",
    "\n",
    "A **false positive** (FP) occurs when we incorrectly predict the positice class.\n",
    "\n",
    "A **false negative** (FN) occurs when we incorrectly predict the negative class.\n",
    "\n",
    "These concepts can be taken together to produce many different aspects of a classifier that we may care about.\n",
    "\n",
    "**accuracy** - Overall, how often is the classifier right.\n",
    ":\t`sklearn.metrics.accuracy_score`\n",
    "\n",
    "$$\\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "**precision**\n",
    ":\t`sklearn.metrics.precision_score`\n",
    "\n",
    "$$\\frac{TP}{TP + FP}$$\n",
    "\n",
    "**recall** (sensitivity)\n",
    ":\t`sklearn.metrics.recall_score`\n",
    "\n",
    "$$\\frac{TP}{TP + FN}$$\n",
    "\n",
    "**roc_auc** - The area under the receiver operating characteristic (ROC) curve. Measure of the trade-off between the true positive rate (TPR) and the false positive rate (FPR) as we vary the classification threshold. Equivalently, it's the probability that an observation drawn at random is classified correctly.\n",
    ":\t`sklearn.metrics.roc_auc_score`\n",
    "\n",
    "**f1**\n",
    ":\t`sklearn.metrics.f1_score` - the harmonic mean between recall and precision\n",
    "\n",
    "$$\\frac{2TP}{2TP + FP + FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Consider the following generated data. Compute the above measures, using any classifier we have seen so far. Use `sklearn.metrics.confusion_matrix` to calculate the quantities by hand. Confirm that they are correct by using the functions noted above.\n",
    "\n",
    "\n",
    "* What is the accuracy of this classifier?\n",
    "* The precision and recall?\n",
    "* The ROC AUC score?\n",
    "* The f1-measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1000, \n",
    "    n_features=50, \n",
    "    n_informative=25, \n",
    "    n_redundant=0, \n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "X_train = X[:750]\n",
    "y_train = y[:750]\n",
    "\n",
    "X_test = X[750:]\n",
    "y_test = y[750:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold Cross-validation\n",
    "\n",
    "As introduced above, cross-validation is probably the most widely used method for estimating generalization error. In particularly data rich environments, we may split our sample into a training set, a testing set, and a validation set that is completely set aside until we have performed model selection.\n",
    "\n",
    "**K-fold cross-validation** is the next best thing. In k-folds cross-validation, the training set is split into *k* smaller sets. Then, for each of the k \"folds\":\n",
    "\n",
    "1. trained model on *k-1* of the folds as training data\n",
    "2. validate this model the remaining fold, using an appropriate metric\n",
    "\n",
    "The performance measure reported by k-fold CV is then the average of the *k* computed values. This approach can be computationally expensive, but does not waste too much data, which is an advantage over having a fixed test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "nfolds = 3\n",
    "kf = KFold(n_splits=nfolds, shuffle=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, nfolds, figsize=(14,4))\n",
    "\n",
    "for i, fold in enumerate(kf.split(salmon.values)):\n",
    "    training, validation = fold\n",
    "    y, x = salmon.values[training].T\n",
    "    axes[i].plot(x, y, 'ro', label='training')\n",
    "    y, x = salmon.values[validation].T\n",
    "    axes[i].plot(x, y, 'bo', label='validation')\n",
    "    axes[i].legend(fontsize='large')\n",
    "    \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "degrees = np.arange(8)\n",
    "k_fold_err = np.empty(len(degrees))\n",
    "\n",
    "for i, d in enumerate(degrees):\n",
    "    \n",
    "    error = np.empty(k)\n",
    "            \n",
    "    for j, fold in enumerate(KFold(n_splits=k).split(salmon.values)):\n",
    "\n",
    "        training, validation = fold\n",
    "        \n",
    "        y_train, x_train = salmon.values[training].T\n",
    "        y_test, x_test = salmon.values[validation].T\n",
    "        \n",
    "        p = np.polyfit(x_train, y_train, d)\n",
    "        \n",
    "        error[j] = rmse(x_test, y_test, p)\n",
    "\n",
    "    k_fold_err[i] = error.mean()\n",
    "        \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(degrees, k_fold_err, lw=2)\n",
    "ax.set_xlabel('degree of fit')\n",
    "ax.set_ylabel('average rms error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model shows high **bias**, the following actions might help:\n",
    "\n",
    "- **Add more features**. In our example of predicting home prices,\n",
    "  it may be helpful to make use of information such as the neighborhood\n",
    "  the house is in, the year the house was built, the size of the lot, etc.\n",
    "  Adding these features to the training and test sets can improve\n",
    "  a high-bias estimator\n",
    "- **Use a more sophisticated model**. Adding complexity to the model can\n",
    "  help improve on bias. For a polynomial fit, this can be accomplished\n",
    "  by increasing the degree d. Each learning technique has its own\n",
    "  methods of adding complexity.\n",
    "- **Decrease regularization**. Regularization is a technique used to impose\n",
    "  simplicity in some machine learning models, by adding a penalty term that\n",
    "  depends on the characteristics of the parameters. If a model has high bias,\n",
    "  decreasing the effect of regularization can lead to better results.\n",
    "  \n",
    "If the model shows **high variance**, the following actions might help:\n",
    "\n",
    "- **Use fewer features**. Using a feature selection technique may be\n",
    "  useful, and decrease the over-fitting of the estimator.\n",
    "- **Use a simpler model**.  Model complexity and over-fitting go hand-in-hand.\n",
    "- **Use more training samples**. Adding training samples can reduce\n",
    "  the effect of over-fitting, and lead to improvements in a high\n",
    "  variance estimator.\n",
    "- **Increase regularization**. Regularization is designed to prevent\n",
    "  over-fitting. In a high-variance model, increasing regularization\n",
    "  can lead to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap aggregating regression\n",
    "\n",
    "Splitting datasets into training, cross-validation and testing subsets is inefficient, particularly when the original dataset is not large. As an alternative, we can use bootstrapping to both develop and validate our model without dividing our dataset. One algorithm to facilitate this is the **bootstrap aggreggation** (or *bagging*) algorithm.\n",
    "\n",
    "The **bootstrap** is a tool for assessing statistical accuracy. It involves sampling training, target pairs *with replacement* from the original dataset $B$ times. Each sample is the same size as the original dataset.\n",
    "\n",
    "A Bagging regressor is an **ensemble meta-estimator** that fits base regressors each on bootstrapped random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X,y = salmon.values.T\n",
    "br = BaggingRegressor(LinearRegression(), oob_score=True, random_state=20090425)\n",
    "X2 = PolynomialFeatures(degree=2).fit_transform(X[:, None])\n",
    "br.fit(X2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate a particular model, the samples that were not selected for a particular resampled dataset (the **out-of-bag** sample) can be used to estimate the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn includes a convenient facility called a **pipeline**, which can be used to chain two or more estimators into a single function. We will hear more about using pipelines later.  \n",
    "\n",
    "Here, we will use it to join the bagging regressor, polynomial feature selection, and linear regression into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def polynomial_bagging_regression(degree, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree=degree),\n",
    "                         BaggingRegressor(LinearRegression(), **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for d in degrees:\n",
    "    \n",
    "    print('fitting', d)\n",
    "    pbr = polynomial_bagging_regression(d, oob_score=True)\n",
    "    pbr.fit(X[:, None], y)\n",
    "    scores.append(pbr.score(X[:, None], y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "The `scikit-learn` package includes a built-in dataset of diabetes progression, taken from [Efron *et al.* (2003)](http://arxiv.org/pdf/math/0406456.pdf), which includes a set of 10 normalized predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Predictors: \"age\" \"sex\" \"bmi\" \"map\" \"tc\"  \"ldl\" \"hdl\" \"tch\" \"ltg\" \"glu\"\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how a linear regression model performs across a range of sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diabetes['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "def plot_learning_curve(estimator, label=None):\n",
    "    scores = list()\n",
    "    train_sizes = np.linspace(10, 200, 10).astype(np.int)\n",
    "    for train_size in train_sizes:\n",
    "        test_error = model_selection.cross_val_score(estimator, diabetes['data'], diabetes['target'],\n",
    "                        cv=model_selection.ShuffleSplit(train_size=train_size, \n",
    "                                                         test_size=200, \n",
    "                                                         random_state=0)\n",
    "                        )\n",
    "        scores.append(test_error)\n",
    "\n",
    "    plt.plot(train_sizes, np.mean(scores, axis=1), label=label or estimator.__class__.__name__)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Explained variance on test set')\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.legend(loc='best', fontsize='x-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the linear regression is not defined for scenarios where the number of features/parameters exceeds the number of observations. It performs poorly as long as the number of sample is not several times the number of features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach for dealing with overfitting is to **regularize** the regession model.\n",
    "\n",
    "The **ridge estimator** is a simple, computationally efficient regularization for linear regression.\n",
    "\n",
    "$$\\hat{\\beta}^{ridge} = \\text{argmin}_{\\beta}\\left\\{\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^k x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^k \\beta_j^2 \\right\\}$$\n",
    "\n",
    "Typically, we are not interested in shrinking the mean, and coefficients are **standardized** to have zero mean and unit L2 norm. Hence,\n",
    "\n",
    "$$\\hat{\\beta}^{ridge} = \\text{argmin}_{\\beta} \\sum_{i=1}^N (y_i - \\sum_{j=1}^k x_{ij} \\beta_j)^2$$\n",
    "\n",
    "$$\\text{subject to } \\sum_{j=1}^k \\beta_j^2 < \\lambda$$\n",
    "\n",
    "Note that this is *equivalent* to a Bayesian model $y \\sim N(X\\beta, I)$ with a Gaussian prior on the $\\beta_j$:\n",
    "\n",
    "$$\\beta_j \\sim \\text{N}(0, \\lambda)$$\n",
    "\n",
    "The estimator for the ridge regression model is:\n",
    "\n",
    "$$\\hat{\\beta}^{ridge} = (X'X + \\lambda I)^{-1}X'y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "k = diabetes['data'].shape[1]\n",
    "alphas = np.linspace(0, 4)\n",
    "params = np.zeros((len(alphas), k))\n",
    "\n",
    "for i,a in enumerate(alphas):\n",
    "    X = preprocessing.scale(diabetes['data'])\n",
    "    y = diabetes['target']\n",
    "    \n",
    "    fit = Ridge(alpha=a, normalize=True).fit(X, y)\n",
    "    params[i] = fit.coef_\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(14,6))\n",
    "for param in params.T:\n",
    "    ax.plot(alphas, param)\n",
    "    ax.set_xlabel(\"$\\\\alpha$\")\n",
    "    ax.set_ylabel(\"$\\\\beta$\", rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(LinearRegression())\n",
    "plot_learning_curve(Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that at very small sample sizes, the ridge estimator outperforms the unregularized model.\n",
    "\n",
    "The regularization of the ridge is a **shrinkage**: the coefficients learned are shrunk towards zero.\n",
    "\n",
    "The amount of regularization is set via the `alpha` parameter of the ridge, which is tunable. The `RidgeCV` method in `scikits-learn` automatically tunes this parameter via cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a in [0.001, 0.01, 0.1, 1, 10]:\n",
    "    plot_learning_curve(Ridge(a), a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn's `RidgeCV` class automatically tunes the L2 penalty using Generalized Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "plot_learning_curve(LinearRegression())\n",
    "plot_learning_curve(Ridge())\n",
    "plot_learning_curve(RidgeCV())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the Ridge estimator, **the Lasso estimator** is useful to impose sparsity on the coefficients. In other words, it is to be prefered if we believe that many of the features are not relevant.\n",
    "\n",
    "$$\\hat{\\beta}^{lasso} = \\text{argmin}_{\\beta}\\left\\{\\frac{1}{2}\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^k x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^k |\\beta_j| \\right\\}$$\n",
    "\n",
    "or, similarly:\n",
    "\n",
    "$$\\hat{\\beta}^{lasso} = \\text{argmin}_{\\beta} \\frac{1}{2}\\sum_{i=1}^N (y_i - \\sum_{j=1}^k x_{ij} \\beta_j)^2$$\n",
    "$$\\text{subject to } \\sum_{j=1}^k |\\beta_j| < \\lambda$$\n",
    "\n",
    "Note that this is *equivalent* to a Bayesian model $y \\sim N(X\\beta, I)$ with a **Laplace** prior on the $\\beta_j$:\n",
    "\n",
    "$$\\beta_j \\sim \\text{Laplace}(\\lambda) = \\frac{\\lambda}{2}\\exp(-\\lambda|\\beta_j|)$$\n",
    "\n",
    "Note how the Lasso imposes sparseness on the parameter coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "k = diabetes['data'].shape[1]\n",
    "alphas = np.linspace(0.1, 3)\n",
    "params = np.zeros((len(alphas), k))\n",
    "for i,a in enumerate(alphas):\n",
    "    X = preprocessing.scale(diabetes['data'])\n",
    "    y = diabetes['target']\n",
    "    \n",
    "    fit = Lasso(alpha=a, normalize=True).fit(X, y)\n",
    "    params[i] = fit.coef_\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "for param in params.T:\n",
    "    plt.plot(alphas, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(RidgeCV())\n",
    "plot_learning_curve(Lasso(0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the ridge estimator performs better than the lasso, but when there are fewer observations, the lasso matches its performance. Otherwise, the variance-reducing effect of the lasso regularization is unhelpful relative to the increase in bias.\n",
    "\n",
    "With the lasso too, me must tune the regularization parameter for good performance. There is a corresponding `LassoCV` function in `scikit-learn`, but it is computationally expensive. To speed it up, we can reduce the number of values explored for the alpha parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "plot_learning_curve(RidgeCV())\n",
    "plot_learning_curve(LassoCV(n_alphas=10, max_iter=5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't decide? **ElasticNet** is a compromise between lasso and ridge regression.\n",
    "\n",
    "$$\\hat{\\beta}^{elastic} = \\text{argmin}_{\\beta}\\left\\{\\frac{1}{2}\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^k x_{ij} \\beta_j)^2 + (1 - \\alpha) \\sum_{j=1}^k \\beta^2_j + \\alpha \\sum_{j=1}^k |\\beta_j| \\right\\}$$\n",
    "\n",
    "where $\\alpha = \\lambda_1/(\\lambda_1 + \\lambda_2)$. Its tuning parameter $\\alpha$ (`l1_ratio` in `scikit-learn`) controls this mixture: when set to 0, ElasticNet is a ridge regression, when set to 1, it is a lasso. The sparser the coefficients, the higher we should set $\\alpha$. \n",
    "\n",
    "Note that $\\alpha$ can also be set by cross-validation, though it is computationally costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "plot_learning_curve(RidgeCV())\n",
    "plot_learning_curve(ElasticNetCV(l1_ratio=.7, n_alphas=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cross-validation for Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, -1, 20)\n",
    "\n",
    "scores = np.empty(len(alphas))\n",
    "scores_std = np.empty(len(alphas))\n",
    "\n",
    "for i,alpha in enumerate(alphas):\n",
    "    lasso.alpha = alpha\n",
    "    s = model_selection.cross_val_score(lasso, diabetes.data, diabetes.target, n_jobs=-1)\n",
    "    scores[i] = s.mean()\n",
    "    scores_std[i] = s.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.semilogx(alphas, scores)\n",
    "plt.semilogx(alphas, np.array(scores) + np.array(scores_std)/20, 'b--')\n",
    "plt.semilogx(alphas, np.array(scores) - np.array(scores_std)/20, 'b--')\n",
    "plt.yticks(())\n",
    "plt.ylabel('CV score')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "plt.text(5e-2, np.max(scores)+1e-4, str(np.max(scores).round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checking using Learning Curves\n",
    "\n",
    "A useful way of checking model performance (in terms of bias and/or variance) is to plot learning curves, which illustrates the learning process as your model is exposed to more data. When the dataset is small, it is easier for a model of a particular complexity to be made to fit the training data well. As the dataset grows, we expect the training error to increase (model accuracy decreases). Conversely, a relatively small dataset will mean that the model will not generalize well, and hence the cross-validation score will be lower, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(lasso, \n",
    "                                                 diabetes.data, diabetes.target, \n",
    "                                                 train_sizes=[50, 70, 90, 110, 130], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For models with high bias, training and cross-validation scores will tend to converge at a low value (high error), indicating that adding more data will not improve performance. \n",
    "\n",
    "For models with high variance, there may be a gap between the training and cross-validation scores, suggesting that model performance could be improved with additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = salmon.values.T\n",
    "X2 = PolynomialFeatures(degree=2).fit_transform(X[:, None])\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(LinearRegression(), X2, y, \n",
    "                                            train_sizes=[10, 15, 20, 30], cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "plt.grid()\n",
    "\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Very low birthweight infants\n",
    "\n",
    "Compare logistic regression models (using the `linear_model.LogisticRegression` interface) with varying degrees of regularization for the VLBW infant database. Use a relevant metric such as the Brier's score as a metric.\n",
    "\n",
    "$$B = \\frac{1}{n} \\sum_{i=1}^n (\\hat{p}_i - y_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vlbw = pd.read_csv(\"../data/vlbw.csv\", index_col=0)\n",
    "\n",
    "vlbw = vlbw.replace(\n",
    "    {\n",
    "        'inout': {\n",
    "            'born at Duke': 0,\n",
    "            'transported': 1\n",
    "    },\n",
    "        'delivery': {\n",
    "            'abdominal':0,\n",
    "            'vaginal':1\n",
    "        },\n",
    "        'ivh': {\n",
    "            'absent': 0, \n",
    "            'present': 1,\n",
    "            'possible': 1,\n",
    "            'definite': 1\n",
    "        }, \n",
    "        'sex': {\n",
    "            'female': 0,\n",
    "            'male': 1\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "vlbw = vlbw[['birth', 'exit', 'hospstay', 'lowph', 'pltct', \n",
    "             'bwt', 'gest', 'meth', 'toc', 'delivery', 'apg1', \n",
    "             'vent', 'pneumo', 'pda', 'cld', 'ivh']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "- Burnham, K. P., & Anderson, D. R. (2002). [Model Selection and Multi-Model Inference: A Practical, Information-theoretic Approach](http://www.amazon.com/Model-Selection-Multimodel-Inference-Information-Theoretic/dp/0387953647). Springer Verlag.\n",
    "- Givens, G. H.; Hoeting, J. A. (2012). [Computational Statistics](http://www.stat.colostate.edu/computationalstatistics/) (Wiley Series in Computational Statistics)\n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). [The elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/). Springer Verlag.\n",
    "- Vanderplas, J. [Scikit-learn tutorials for the Scipy 2013 conference](https://github.com/jakevdp/sklearn_scipy2013)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
