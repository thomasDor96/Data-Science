{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Neural Networks in PyMC3\n",
    "\n",
    "Bayesian deep learning combines deep neural networks with probabilistic methods to provide information about the **uncertainty** associated with its predictions. Not only is accounting for prediction uncertainty important for real-world applications, it is also be useful in training. For example, we could train the model specifically on samples it is most uncertain about.\n",
    "\n",
    "We can also quantify the uncertainty in our estimates of network weights, which could inform us about the **stability** of the learned representations of the network.\n",
    "\n",
    "In classical neural networks, weights are often L2-regularized to avoid overfitting, which corresponds exactly to Gaussian priors over the weight coefficients. We could, however, imagine all kinds of other priors, like spike-and-slab to enforce sparsity (this would be more like using the L1-norm).\n",
    "\n",
    "If we wanted to train a network on a new object recognition data set, we could bootstrap the learning by placing informed priors centered around weights retrieved from other pre-trained networks, like [GoogLeNet](https://arxiv.org/abs/1409.4842). \n",
    "\n",
    "Additionally, a very powerful approach in Probabilistic Programming is **hierarchical modeling**, which allows pooling of things that were learned on sub-groups to the overall population. Applied here, individual neural nets can be applied to sub-groups based on sharing information from the overall population. \n",
    "\n",
    "Let's generate another simulated classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks')\n",
    "\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import pymc3 as pm\n",
    "from scipy import optimize\n",
    "from ipywidgets import interact\n",
    "from IPython.display import SVG\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "floatX = theano.config.floatX\n",
    "theano.config.compute_test_value = 'ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(noise=0.2, n_samples=1000)\n",
    "X = scale(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[y==0, 0], X[y==0, 1], label='Class 0')\n",
    "ax.scatter(X[y==1, 0], X[y==1, 1], color='r', label='Class 1')\n",
    "sns.despine(); ax.legend()\n",
    "ax.set(xlabel='X1', ylabel='X2', title='Toy binary classification data set');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaling performed above should result in faster training.\n",
    "\n",
    "We first create training and test sets, and convert the training set to Theano tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(floatX)\n",
    "y = y.astype(floatX)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "\n",
    "ann_input = theano.shared(X_train)\n",
    "ann_output = theano.shared(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using standard normal deviates for initial values will facilitate convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 5\n",
    "\n",
    "init_1 = np.random.randn(X.shape[1], n_hidden).astype(floatX)\n",
    "init_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
    "init_out = np.random.randn(n_hidden).astype(floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use 2 hidden layers with 5 neurons each which is sufficient for such a simple problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as neural_network:\n",
    "    \n",
    "    # Weights from input to hidden layer\n",
    "    weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n",
    "                             shape=(X.shape[1], n_hidden), \n",
    "                             testval=init_1)\n",
    "\n",
    "    # Weights from 1st to 2nd layer\n",
    "    weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n",
    "                            shape=(n_hidden, n_hidden), \n",
    "                            testval=init_2)\n",
    "\n",
    "    # Weights from hidden layer to output\n",
    "    weights_2_out = pm.Normal('w_2_out', 0, sd=1, \n",
    "                              shape=(n_hidden,), \n",
    "                              testval=init_out)\n",
    "\n",
    "    # Build neural-network using tanh activation function\n",
    "    act_1 = pm.math.tanh(pm.math.dot(ann_input, \n",
    "                                     weights_in_1))\n",
    "    act_2 = pm.math.tanh(pm.math.dot(act_1, \n",
    "                                     weights_1_2))\n",
    "    act_out = pm.math.sigmoid(pm.math.dot(act_2, \n",
    "                                          weights_2_out))\n",
    "\n",
    "    # Binary classification -> Bernoulli likelihood\n",
    "    out = pm.Bernoulli('out', \n",
    "                       act_out,\n",
    "                       observed=ann_output,\n",
    "                       total_size=y_train.shape[0] # IMPORTANT for minibatches\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use Markov chain Monte Carlo sampling, which works pretty well in this case, but this will become very slow as we scale our model up to deeper architectures with more layers.\n",
    "\n",
    "Instead, we will use the the ADVI variational inference algorithm. This is much faster and will scale better. Note, that this is a mean-field approximation so we ignore correlations in the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with neural_network:\n",
    "    approx = pm.fit(n=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As samples are more convenient to work with, we can very quickly draw samples from the variational approximation using the `sample` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = approx.sample(draws=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the objective function (ELBO) we can see that the optimization slowly improves the fit over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(-approx.hist, alpha=.3)\n",
    "plt.legend()\n",
    "plt.ylabel('ELBO')\n",
    "plt.xlabel('iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained our model, lets predict on the hold-out set using a posterior predictive check (PPC). \n",
    "\n",
    "1. We can use [`sample_ppc()`](../api/inference.rst) to generate new data (in this case class predictions) from the posterior (sampled from the variational estimation).\n",
    "2. To improve performance, is better to get the node directly and build theano graph using our approximation (`approx.sample_node`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create symbolic input\n",
    "x = tt.matrix('X')\n",
    "# symbolic number of samples is supported, we build vectorized posterior on the fly\n",
    "n = tt.iscalar('n')\n",
    "# Do not forget test_values or set theano.config.compute_test_value = 'off'\n",
    "x.tag.test_value = np.empty_like(X_train[:10])\n",
    "n.tag.test_value = 100\n",
    "_sample_proba = approx.sample_node(neural_network.out.distribution.p, \n",
    "                                   size=n,\n",
    "                                   more_replacements={ann_input: x})\n",
    "# It is time to compile the function\n",
    "# No updates are needed for Approximation random generator \n",
    "# Efficient vectorized form of sampling is used\n",
    "sample_proba = theano.function([x, n], _sample_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sample_proba(X_test, 500).mean(0) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy = {:0.1f}%'.format((y_test == pred).mean() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\n",
    "ax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\n",
    "sns.despine()\n",
    "ax.set(title='Predicted labels in testing set', xlabel='X1', ylabel='X2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what the classifier has learned.\n",
    "\n",
    "For this, we evaluate the class probability predictions on a grid over the whole input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pm.floatX(np.mgrid[-3:3:100j,-3:3:100j])\n",
    "grid_2d = grid.reshape(2, -1).T\n",
    "dummy_out = np.ones(grid.shape[1], dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppc = sample_proba(grid_2d ,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a probability surface corresponding to the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "contour = ax.contourf(grid[0], grid[1], ppc.mean(axis=0).reshape(100, 100), cmap=cmap)\n",
    "ax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\n",
    "ax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X1', ylabel='X2');\n",
    "cbar.ax.set_ylabel('Posterior predictive mean probability of class label = 0');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, unlike a classical neural network, we can also look at the standard deviation of the posterior predictive to get a sense for the uncertainty in our predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = sns.cubehelix_palette(light=1, as_cmap=True)\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "contour = ax.contourf(grid[0], grid[1], ppc.std(axis=0).reshape(100, 100), cmap=cmap)\n",
    "ax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\n",
    "ax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\n",
    "cbar.ax.set_ylabel('Uncertainty (posterior predictive standard deviation)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "T. Wiecki and M. Kochurov. (2017) [Variational Inference: Bayesian Neural Networks](http://docs.pymc.io/notebooks/bayesian_neural_network_advi.html)\n",
    "\n",
    "D. Rodriguez. (2013) [Basic [1 hidden layer] neural network on Python](http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/).\n",
    "\n",
    "D. Britz. (2015) [Implementing a Neural Network from Scratch](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
