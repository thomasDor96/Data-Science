{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Regression and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it may not be apparent how to use trees for regression analysis, it requires only a straightforward modification to the algorithm. A popular tree-based regression algorithm is the **classification and regression tree** (CART).\n",
    "\n",
    "The file `TNNASHVI.txt` in your data directory contains daily temperature readings for Nashville, courtesy of the [Average Daily Temperature Archive](http://academic.udayton.edu/kissock/http/Weather/). This data, as one would expect, oscillates annually. We can use a decision tree regression model to fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_temps = pd.read_table(\"../data/TNNASHVI.txt\", sep='\\s+', \n",
    "                            names=['month','day','year','temp'], na_values=-99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_temps.temp[daily_temps.year>2010].plot(style='b.', figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, none of the cost functions considered so far would be appropriate. Instead, it would be more suitable to use something like mean squared error (MSE) to guide the growth of the tree. With this, we can proceed to choose:\n",
    "\n",
    "1. a variable on which to split the dataset\n",
    "2. a value of the variable at which to place a node (in the case of continuous features)\n",
    "\n",
    "Recall that the output of a tree is just a constant value for each leaf; here, we simply return the average of all the response values in the region. Thus, we choose a cut point that minimizes the MSE at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transmogrify data\n",
    "y = daily_temps.temp[daily_temps.year>2010]\n",
    "X = np.atleast_2d(np.arange(len(y))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "clf = DecisionTreeRegressor(max_depth=7, min_samples_leaf=2)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.linspace(0, len(X), 1000).reshape((-1, 1))\n",
    "y_fit_1 = clf.predict(X_fit)\n",
    "\n",
    "plt.plot(X.ravel(), y, '.k', alpha=0.3)\n",
    "plt.plot(X_fit.ravel(), y_fit_1, color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single decision tree allows us to estimate the signal in a non-parametric way,\n",
    "but clearly has some issues.  In some regions, the model shows high bias and\n",
    "under-fits the data\n",
    "(seen in the long flat lines which don't follow the contours of the data),\n",
    "while in other regions the model shows high variance and over-fits the data\n",
    "(reflected in the narrow spikes which are influenced by noise in single points).\n",
    "\n",
    "One way to address this is to use an **ensemble method**, so that the\n",
    "effects of their over-fitting go away on average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Another effective ensemble method for classification and regression is gradient boosting, or **boosting**. Boosting was originally derived in the computational learning theory literature, where it was proved that one could boost the performance of any \"weak\" learner arbitrarily high, provided the weak learner could always perform slightly better than chance. The idea is that by sequentially applying very fast, simple models, we can get a total model error which is better than any of the individual pieces. Hastie and Tibshirani describe it as:\n",
    "\n",
    "> one of the most powerful learning ideas introduced in the last twenty years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "One of the simplest and most prevalent boosting algorithms is the **AdaBoost.M1** method by [Freund and Schapire (1997)](http://www.sciencedirect.com/science/article/pii/S002200009791504X), which aims to improve the accuracy of *any* learning algorithm.\n",
    "\n",
    "If we consider a binary output, coded as $Y \\in \\{-1,1\\}$. We can define the error rate of some classifier $G(X)$ as:\n",
    "\n",
    "$$\\overline{err} = \\frac{1}{N} \\sum_{i=1}^N I[y_i \\ne G(x_i)]$$\n",
    "\n",
    "A classifier is called a *weak* classifier when its error rate is only marginally better than random selection of outcomes. Boosting takes a weak classifier and applies it sequentially to data which have been weighted by having been passed through previous iterations of the classifier. A final prediction is produced by a weighted majority vote from all the classifiers.\n",
    "\n",
    "![adaboost](http://d.pr/i/1bTsQ+)\n",
    "\n",
    "Thus, the $\\alpha_m$ values are calculated by the classifier, and are used to weight the estimates in generating a final prediction. Observations that are misclassified by a given classifier are weighted more heavily so that they are given more emphasis in subsequent classification steps. Hence, observations that are more troublesome are given increasing influence, until they are classified correctly.\n",
    "\n",
    "### AdaBoost.M1\n",
    "\n",
    "1. Initialize observation weights uniformly: $w_i = 1/N$\n",
    "2. Iterate $m=1,\\ldots,M$:\n",
    "    + Fit classifier $G_m(x)$ to trainging data, weighted by $w_i$\n",
    "    + Calculate weighted classification error:\n",
    "    $$\\text{err}_m = \\frac{\\sum_i w_i I[y_i \\ne G_m(x_i)]}{\\sum_i w_i}$$\n",
    "    + Compute $\\alpha_m = \\log[(1-\\text{err}_m)/\\text{err}_m]$\n",
    "    + Recalculate weights: $w_i = w_i \\exp(a_m I[y_i \\ne G_m(x_i)])$\n",
    "3. Return $G(x) = \\text{sign}[\\sum_m \\alpha_m G_m(x)]$\n",
    "\n",
    "\n",
    "Boosting is simply a form of forward stagewise additive modeling. It is a particular type of basis function expansion, of the general form:\n",
    "\n",
    "$$f(x | \\{\\beta_m, \\theta_m\\}) = \\sum_{m=1}^M \\beta_m b(x | \\theta_m)$$\n",
    "\n",
    "where the $\\beta_m$ are expansion coefficients and $b$ some multivariate function parameterized by $\\theta_m$. We saw an example of such basis expansions in the previous section on decision trees, where the $b$ are internal nodes and the predictions are terminal nodes.\n",
    "\n",
    "These models are fit by minimizing some loss function:\n",
    "\n",
    "$$\\min_{\\{\\beta_m, \\theta_m\\}} \\sum_{i=1}^N L(y_i, f(x | \\{\\beta_m, \\theta_m\\})$$\n",
    "\n",
    "We can approximate the optimization by adding the next basis function to a set of existing functions, without adjusting the parameters of the functions already added.\n",
    "\n",
    "The algorithm is as follows:\n",
    "\n",
    "1. Initialize $f_0(x) = 0$\n",
    "2. Iterate $m=1,\\ldots,M$:\n",
    "    + Optimize:\n",
    "    $$(\\beta_m, \\theta_m) = \\text{argmin}_{\\beta, \\theta} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i) + \\beta_m b(x|\\theta_m))$$\n",
    "    + Set $f_m(x) = f_{m-1}(x) + \\beta_m b(x|\\theta_m)$\n",
    "    \n",
    "We can use a function such as squared-error loss.\n",
    "\n",
    "Returning to AdaBoost.M1, we can use an exponential loss function:\n",
    "\n",
    "$$L(y, f(x)) = \\exp(-y \\alpha G(x))$$\n",
    "\n",
    "which means we need to calculate:\n",
    "\n",
    "$$(\\alpha_m, G_m) = \\text{argmin}_{\\alpha, G} \\sum_{i=1}^N w_i^{(m)} \\exp[-y_i \\alpha G(x_i)]$$\n",
    "\n",
    "where the weights are:\n",
    "\n",
    "$$w_i^{(m)} = \\exp[-y_i f_{m-1}(x)] $$\n",
    "\n",
    "Let's implement this algorithm in Python, and run it on some artificial data, which consists of two predictors, simulated from a uniform random number generator, with the class label set to 1 where both values are greater than 0.4, and -1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "ndata = 50\n",
    "\n",
    "train_data = np.random.rand(2, ndata)\n",
    "train_classes = np.where(((train_data[0,:]>0.4) & (train_data[1,:]>0.4)),1,-1)\n",
    "\n",
    "test_data = np.random.rand(2,ndata)\n",
    "test_classes = np.where(((test_data[0,:]>0.4) & (test_data[1,:]>0.4)),1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_data[0, np.where(train_classes==-1)], train_data[1, np.where(train_classes==-1)],'ro')\n",
    "plt.plot(train_data[0, np.where(train_classes==1)], train_data[1, np.where(train_classes==1)],'bo')\n",
    "plt.title('Training Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_data[0, np.where(test_classes==-1)], test_data[1, np.where(test_classes==-1)],'ro')\n",
    "plt.plot(test_data[0, np.where(test_classes==1)], test_data[1, np.where(test_classes==1)],'bo')\n",
    "plt.title('Test Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a very simple weak classifier that generates a class label by fitting only one of the two predictor variables, based on a threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(data, true_classes, value):\n",
    "    \n",
    "    # Classify\n",
    "    classes = np.where(data<value, -1, 1)\n",
    "    \n",
    "    # Misclassification\n",
    "    wrong = np.where(true_classes!=classes, 1, 0)\n",
    "    \n",
    "    return classes, wrong\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(train_data[0], train_classes, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training function calculates the error over a range of classifier values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, true_classes, weights):\n",
    "        \n",
    "    error = np.zeros(10)\n",
    "    \n",
    "    for value in range(10):\n",
    "        \n",
    "        val = value/10.\n",
    "        classes = np.where(data<val, -1, 1)\n",
    "        error[value] = np.sum(weights[np.where(true_classes!=classes)])\n",
    "\n",
    "    return np.argmin(error)/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data[1], train_classes, np.ones(len(train_data[0]), float)/len(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below runs the AdaBoost algorithm, with an exponential loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "def adaboost(x_train, y_train, x_test, y_test, M=20):\n",
    "    \n",
    "    # Number of variables and training points\n",
    "    K, N = np.shape(x_train)\n",
    "    \n",
    "    classifier = np.zeros(M)\n",
    "    alpha = np.zeros(M)\n",
    "    var = np.zeros(M, int)\n",
    "    \n",
    "    # Initial model weights\n",
    "    w = np.ones(N, float)/N\n",
    "\n",
    "    train_errors = np.zeros(M)\n",
    "    test_errors = np.zeros(M)\n",
    "    \n",
    "    pred_train = np.zeros((M, N))\n",
    "    pred_test = np.zeros((M, N))\n",
    "    \n",
    "    for m in range(M):\n",
    "        \n",
    "        # Pick random variable to use as classifier\n",
    "        var[m] = np.random.binomial(1, 0.5)\n",
    "        \n",
    "        # Run classifier on variable using training x_train\n",
    "        classifier[m] = train(x_train[var[m]], y_train, w)\n",
    "        \n",
    "        # Classify training and test x_train\n",
    "        train_classes, train_error = classify(x_train[var[m]], y_train, classifier[m])\n",
    "        test_classes, test_error = classify(x_test[var[m]], y_test, classifier[m])\n",
    "\n",
    "        weighted_error = np.sum(w*train_error)/w.sum()\n",
    "            \n",
    "        if m and (weighted_error==0 or weighted_error>=0.5):\n",
    "            M=m\n",
    "            print('exiting at',m)\n",
    "            break\n",
    "\n",
    "        alpha[m] = np.log((1-weighted_error)/weighted_error)\n",
    "        \n",
    "        # Update weights\n",
    "        w *= np.exp(alpha[m]*train_error)\n",
    "        w /= np.sum(w)\n",
    "\n",
    "        train_classes = np.zeros((N, m))\n",
    "        test_classes = np.zeros((N, m))\n",
    "        for i in range(m):\n",
    "            train_classes[:, i], _ = classify(x_train[var[i]], y_train, classifier[i])\n",
    "            test_classes[:, i], _ = classify(x_test[var[i]], y_test, classifier[i])\n",
    "\n",
    "        # Make prediction based on series of boosted weak learners\n",
    "        for n in range(N):\n",
    "            pred_train[m, n] = np.sum(alpha[:m]*train_classes[n])/alpha.sum()\n",
    "            pred_test[m, n] = np.sum(alpha[:m]*test_classes[n])/alpha.sum() \n",
    "            \n",
    "        # Classify based on the sum of the weighted classes\n",
    "        pred_train_classes = np.where(pred_train[m]>0, 1, -1)\n",
    "        pred_test_classes = np.where(pred_test[m]>0, 1, -1)\n",
    "        \n",
    "        train_errors[m] = sum(pred_train_classes!=y_train)\n",
    "        test_errors[m] = sum(pred_test_classes!=y_test)\n",
    "    \n",
    "    plt.plot(np.arange(M), train_errors[:M]/N,'k-', np.arange(M), test_errors[:M]/N,'k--')\n",
    "    plt.legend(('Training error','Test error'))\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost(train_data, train_classes, test_data, test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the algorithm takes what is a weak classifier and iteratively builds a strong ensemble classifier.\n",
    "\n",
    "A similar approach can be applied to decision trees, whereby the simplest possible tree (depth=1) is used as a weak classifier at each step. This is often *worse* than random chance for classification at the outset, but when boosting is applied, the resulting classifier can be powerful. This approach to ensemble decision tree learning is called **stumping**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Trees\n",
    "\n",
    "Recall that for decision trees, we partition the space of all predictor variables into disjoint regions $R_j$, where some constant value $\\gamma_j$ (e.g. a mean) is returned for any points in the parameter space located within $R_j$. Formally, we can express a tree as:\n",
    "\n",
    "$$T(x|\\{R_j, \\gamma_j\\}) = \\sum_{j=1}^J \\gamma_j I(x \\in R_j)$$\n",
    "\n",
    "The optimization required to find the best parameters is computationally difficult, and we typically fall back on growing and pruning strategies to obtain a competitive solution that is not guaranteed to be optimal.\n",
    "\n",
    "An alternative approach is to apply boosting to tree-building, which is just a sum of trees:\n",
    "\n",
    "$$f_M(x) = \\sum_{m=1}^M T(x| \\{ R_{j}, \\gamma_{j} \\}_m)$$\n",
    "\n",
    "induced in a stagewise manner. Specifically, at each step we solve:\n",
    "\n",
    "$$(R_m, \\gamma_m) = \\text{argmin}_{R, \\gamma} \\sum_{i=1}^N L(y_i, f_{m-1}(x_i) + T(x_i| \\{R_j, \\gamma_j\\}_m))$$\n",
    "\n",
    "where $L$ is a chosen loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression problems, boosting is a form of **functional gradient descent**. Boosting is a numerical optimization technique for minimizing the loss function by adding, at each step, a new tree that best reduces (by stepping down the gradient) the cost function. For boosted regression trees, the first regression tree is the one that, for the selected tree size, maximally reduces the loss function. For subsequent steps, the focus is on the residuals: on variation in the response that is not so far explained by the model.\n",
    "\n",
    "The process is *stagewise*, meaning that existing trees are left unchanged as the model is enlarged. Only the fitted value for each observation is re-estimated at each step to reflect the contribution of the newly added tree. The final model is a linear combination of many trees (usually hundreds to thousands) that can be thought of as a regression model where each term itself is a tree.\n",
    "\n",
    "The example below presents a simple artificial problem, generated from a sinoid function with random Gaussian noise. There are 100 training points (blue) for fitting the model and 100 test data points (red) which for evaluating the approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "def true_model(x):\n",
    "    return x * np.sin(x) + np.sin(2 * x)\n",
    "\n",
    "def gen_data(n_samples=200):\n",
    "    x = np.random.uniform(0, 10, size=n_samples)\n",
    "    x.sort()\n",
    "    y = true_model(x) + 0.75 * np.random.normal(size=n_samples)\n",
    "    train_mask = np.random.randint(0, 2, size=n_samples).astype(np.bool)\n",
    "    x_train, y_train = x[train_mask, np.newaxis], y[train_mask]\n",
    "    x_test, y_test = x[~train_mask, np.newaxis], y[~train_mask]\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "    \n",
    "\n",
    "X_train, X_test, y_train, y_test = gen_data(200)\n",
    "\n",
    "x_plot = np.linspace(0, 10, 500)\n",
    "\n",
    "def plot_data(alpha=0.6):\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    gt = plt.plot(x_plot, true_model(x_plot), alpha=alpha, label='true function')\n",
    "\n",
    "    # plot training and testing data\n",
    "    plt.scatter(X_train, y_train, s=10, alpha=alpha, label='training data')\n",
    "    plt.scatter(X_test, y_test, s=10, alpha=alpha, color='red', label='testing data')\n",
    "    plt.xlim((0, 10))\n",
    "    plt.ylabel('y')\n",
    "    plt.xlabel('x')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides two estimators for gradient boosting: `GradientBoostingClassifier` and `GradientBoostingRegressor`, both are located in the `sklearn.ensemble` submodule. The relevant hyperparameters for these estimators include:\n",
    "\n",
    "  * number of regression trees (`n_estimators`)\n",
    "  * depth of each individual tree (`max_depth`)\n",
    "  * loss function (`loss`)\n",
    "  * learning rate (`learning_rate`)\n",
    "\n",
    "If you fit an individual regression tree to the above data you get a piece-wise constant approximation. The deeper you grow the tree the more constant segments you can accomodate thus the more variance you can capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "plot_data()\n",
    "est = DecisionTreeRegressor(max_depth=1).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n",
    "         label='RT max_depth=1', color='g', alpha=0.9, linewidth=2)\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "plot_data()\n",
    "est = DecisionTreeRegressor(max_depth=1).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n",
    "         label='RT max_depth=1', color='g', alpha=0.9, linewidth=2)\n",
    "\n",
    "est = DecisionTreeRegressor(max_depth=3).fit(X_train, y_train)\n",
    "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n",
    "         label='RT max_depth=3', color='g', alpha=0.7, linewidth=1)\n",
    "\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets examine the effect on the approximation by adding more trees. The `scikit-learn` gradient boosting estimators allow you to evaluate the prediction of a model as a function of the number of trees via the `staged_predict` and `stages_predict_proba` methods. The return a generator that iterates over the predictions as you add more and more trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from itertools import islice\n",
    "\n",
    "plot_data()\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0)\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "ax = plt.gca()\n",
    "first = True\n",
    "\n",
    "# step over prediction as we added 20 more trees.\n",
    "for pred in islice(est.staged_predict(x_plot[:, np.newaxis]), 0, 1000, 20):\n",
    "    \n",
    "    plt.plot(x_plot, pred, color='r', alpha=0.2)\n",
    "    \n",
    "    if first:\n",
    "        ax.annotate('High bias - low variance', xy=(x_plot[x_plot.shape[0] // 2],\n",
    "                                                    pred[x_plot.shape[0] // 2]),\n",
    "                                                    xycoords='data',\n",
    "                                                    xytext=(3, 4), textcoords='data',\n",
    "                                                    arrowprops=dict(arrowstyle=\"->\",\n",
    "                                                                    connectionstyle=\"arc\"))\n",
    "        first = False\n",
    "\n",
    "pred = est.predict(x_plot[:, np.newaxis])\n",
    "plt.plot(x_plot, pred, color='r', label='GBRT max_depth=1')\n",
    "ax.annotate('Low bias - high variance', xy=(x_plot[x_plot.shape[0] // 2],\n",
    "                                            pred[x_plot.shape[0] // 2]),\n",
    "                                            xycoords='data', \n",
    "                                            xytext=(6.25, -6),\n",
    "                                            textcoords='data', \n",
    "                                            arrowprops=dict(arrowstyle=\"->\",\n",
    "                                                            connectionstyle=\"arc\",\n",
    "                                                            color='k'))\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows 50 red lines, each shows the response of the GBRT model after 20 trees have been added. It starts with a very crude approximation that can only fit more-or-less constant functions (ie. _High bias - low variance_) but as we add more trees the more variance our model can capture resulting in the solid red line.\n",
    "\n",
    "We can see that the more trees we add to our GBRT model and the deeper the individual trees are the more variance we can capture thus the higher the complexity of our model. But as usual in machine learning model complexity trades off variance and bias.\n",
    "\n",
    "A *deviance plot* shows the training/testing error (or deviance) as a function of the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = '#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e','#e6ab02','#a6761d','#666666'\n",
    "\n",
    "n_estimators = len(est.estimators_)\n",
    "\n",
    "def deviance_plot(est, X_test, y_test, ax=None, label='', train_color=colors[0], \n",
    "                  test_color=colors[1], alpha=1.0):\n",
    "\n",
    "    test_dev = np.array([est.loss_(y_test, pred) for pred in est.staged_predict(X_test)])\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(8, 5))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    ax.plot(np.arange(n_estimators) + 1, test_dev, color=test_color, label='Test %s' % label, \n",
    "             linewidth=2, alpha=alpha)\n",
    "    ax.plot(np.arange(n_estimators) + 1, est.train_score_, color=train_color, \n",
    "             label='Train %s' % label, linewidth=2, alpha=alpha)\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.set_xlabel('n_estimators')\n",
    "    ax.set_ylim((0, 2))\n",
    "    \n",
    "    return test_dev, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dev, ax = deviance_plot(est, X_test, y_test)\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "# add some annotations\n",
    "ax.annotate('Lowest test error', xy=(test_dev.argmin() + 1, test_dev.min() + 0.02), xycoords='data',\n",
    "            xytext=(150, 1.0), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\", color='k'),\n",
    "            )\n",
    "\n",
    "ann = ax.annotate('', xy=(800, test_dev[799]),  xycoords='data',\n",
    "                  xytext=(800, est.train_score_[799]), textcoords='data',\n",
    "                  arrowprops=dict(arrowstyle=\"<->\", color='k'))\n",
    "ax.text(810, 0.25, 'train-test gap');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green line above shows the training error: it rapidly decreases in the beginning and then gradually slows down but keeps decreasing as we add more and more trees. The testing error (upper line) too decreases rapidly in the beginning but then slows down and reaches its minimum fairly early (~50 trees) and then even starts increasing.\n",
    "\n",
    "An increasing train-test gap is usually a sign of *overfitting*.\n",
    "\n",
    "The great thing about gradient boosting is that it provides a number of ways to control overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "There are three tuning parameters to consider:\n",
    "\n",
    "1. The depth and number of trees (boosting can overfit with very large numbers of deep trees)\n",
    "2. Shrinkage parameter $\\lambda$ (typically 0.01 or 0.001; smaller values require more trees to be effective)\n",
    "3. Randomization\n",
    "\n",
    "#### Tree Structure\n",
    "\n",
    "The depth of the individual trees is one aspect of model complexity. The depth of the trees basically control the degree of feature **interactions** that your model can fit. For example, if you want to capture the interaction between a feature ``latitude`` and a feature ``longitude`` your trees need a depth of at least two to capture this. Unfortunately, the degree of feature interactions is not known in advance but it is usually fine to assume that it is faily low; in practice, a depth of 4-6 usually gives the best results. In scikit-learn you can constrain the depth of the trees using the ``max_depth`` argument.\n",
    "\n",
    "Another way to control the depth of the trees is by enforcing a lower bound on the number of samples in a leaf. This will avoid inbalanced splits where a leaf is formed for just one extreme data point. In scikit-learn you can do this using the argument ``min_samples_leaf``. This is effectively a means to introduce bias into your model with the hope to also reduce variance as shown in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = '#2ca25f', '#99d8c9', '#e34a33', '#fdbb84', '#c51b8a', '#fa9fb5'\n",
    "\n",
    "def fmt_params(params):\n",
    "    return \", \".join(\"{0}={1}\".format(key, val) for key, val in params.items())\n",
    "\n",
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "for params, (test_color, train_color) in [({}, (colors[0], colors[1])),\n",
    "                                          ({'min_samples_leaf': 3},\n",
    "                                           (colors[2], colors[3])),\n",
    "                                         ({'min_samples_leaf': 10},\n",
    "                                           (colors[4], colors[5]))]:\n",
    "    est = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=1, learning_rate=1.0)\n",
    "    est.set_params(**params)\n",
    "    est.fit(X_train, y_train)\n",
    "    \n",
    "    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n",
    "                                 train_color=train_color, test_color=test_color)\n",
    "    \n",
    "ax.annotate('Higher bias', xy=(900, est.train_score_[899]), xycoords='data',\n",
    "            xytext=(600, 0.3), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\",\n",
    "                           color='k'),\n",
    "            )\n",
    "ax.annotate('Lower variance', xy=(900, test_dev[899]), xycoords='data',\n",
    "            xytext=(600, 0.4), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\", \n",
    "                           color='k'),\n",
    "            )\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrinkage\n",
    "\n",
    "The most important regularization technique for GBRT is shrinkage: the idea is basically to do slow learning by strongly shrinking the predictions of each individual tree by some small scalar, the ``learning_rate``. By doing so the model has to re-enforce concepts. A lower ``learning_rate`` requires a higher number of ``n_estimators`` to get to the same level of training error -- so its trading runtime against accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = plt.gca()\n",
    "for params, (test_color, train_color) in [({}, (colors[0], colors[1])),\n",
    "                                          ({'learning_rate': 0.1},\n",
    "                                           (colors[2], colors[3])),\n",
    "                                         ({'learning_rate': 0.01},\n",
    "                                           (colors[4], colors[5]))]:\n",
    "    est = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=1, learning_rate=1.0)\n",
    "    est.set_params(**params)\n",
    "    est.fit(X_train, y_train)\n",
    "    \n",
    "    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n",
    "                                 train_color=train_color, test_color=test_color)\n",
    "    \n",
    "ax.annotate('Requires more trees', xy=(200, est.train_score_[199]), xycoords='data',\n",
    "            xytext=(300, 1.0), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "ax.annotate('Lower test error', xy=(900, test_dev[899]), xycoords='data',\n",
    "            xytext=(600, 0.5), textcoords='data',\n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
    "            )\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "We now have introduced a number of hyperparameters, each of which require selection via optimization, and some of which interact with each other (``learning_rate`` and ``n_estimators``, ``learning_rate`` and ``subsample``, ``max_depth`` and ``max_features``).\n",
    "\n",
    "We usually follow this recipe to tune the hyperparameters for a gradient boosting model:\n",
    "\n",
    "  1. Choose ``loss`` based on your problem at hand (i.e. target metric)\n",
    "  2. Pick ``n_estimators`` as large as (computationally) possible (e.g. 3000).\n",
    "  3. Tune ``max_depth``, ``learning_rate``, ``min_samples_leaf``, and ``max_features`` via grid search.\n",
    "  4. Increase ``n_estimators`` even more and tune ``learning_rate`` again holding the other parameters fixed.\n",
    "    \n",
    "Scikit-learn provides a convenient API for hyperparameter tuning and grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "              'max_depth': [4, 6],\n",
    "              'min_samples_leaf': [3, 5, 9, 17],\n",
    "              # 'max_features': [1.0, 0.3, 0.1] ## not possible in our example (only 1 fx)\n",
    "              }\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=3000)\n",
    "# this may take some minutes\n",
    "gs_cv = GridSearchCV(est, param_grid, n_jobs=4).fit(X_train, y_train)\n",
    "\n",
    "# best hyperparameter setting\n",
    "gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: California Housing\n",
    "\n",
    "This example shows how to apply GBRT to a real-world dataset. The task is to predict the log median house value for census block groups in California. The dataset is based on the 1990 censues comprising roughly 20.000 groups. There are 8 features for each group including: median income, average house age, latitude, and longitude.\n",
    "\n",
    "We will use Mean Absolute Error as our target metric and evaluate the results on an 80-20 train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.california_housing import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "\n",
    "# split 80/20 train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n",
    "                                                    np.log(cal_housing.target),\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)\n",
    "names = cal_housing.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the aspects that make this dataset challenging are: \n",
    "\n",
    "- heterogenous features (different scales and distributions) \n",
    "- non-linear feature interactions (specifically latitude and longitude)\n",
    "\n",
    "Furthermore, the data contains some extreme values of the response (log median house value). Hence, analysis of this dataset will benefit from robust regression techniques.\n",
    "\n",
    "Below are histograms showing some of the features, along with the response. You can see that they are quite different: median income is left skewed, latitude and longitude are bi-modal, and log median house value is right skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_df = pd.DataFrame(data=X_train, columns=names)\n",
    "X_df['LogMedHouseVal'] = y_train\n",
    "_ = X_df.hist(column=['Latitude', 'Longitude', 'MedInc', 'LogMedHouseVal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit a gradient boosteed regression tree (GBRT) model to this dataset and inspect the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = GradientBoostingRegressor(n_estimators=1000, max_depth=3, learning_rate=0.04,\n",
    "                                loss='huber', random_state=0)\n",
    "est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber loss (`huber`) is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss.\n",
    "\n",
    "$$L_{\\delta}(x) = \\left\\{ \\begin{array}{lr}\n",
    "\\frac{1}{2} x^2 & \\text{for } |x| \\le \\delta\\\\\n",
    "\\delta (|x| - \\frac{1}{2} \\delta) & \\text{otherwise}\n",
    "\\end{array} \\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, est.predict(X_test))\n",
    "print('MAE: %.4f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "\n",
    "Often features do not contribute equally to predict the target response. When interpreting a model, the first question usually is: what are those important features and how do they contributing in predicting the target response?\n",
    "\n",
    "Feature importance in regression trees can be measured by the amount RSS is decreased due to splits over a given predictor, averaged over all trees.\n",
    "\n",
    "A GBRT model derives this information from the fitted regression trees which intrinsically perform feature selection by choosing appropriate split points. You can access this information via the instance attribute ``est.feature_importances_``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort importances\n",
    "indices = np.argsort(est.feature_importances_)\n",
    "\n",
    "# plot as bar chart\n",
    "plt.barh(np.arange(len(names)), est.feature_importances_[indices])\n",
    "plt.yticks(np.arange(len(names)) + 0.25, np.array(names)[indices])\n",
    "_ = plt.xlabel('Relative importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Very low birthweight infants\n",
    "\n",
    "Using the very low birthweight infants dataset we have used previously, build a GBRT to predict the outcome intra-ventricular hemorrhage (`ivh`) from predictors birth weight, gestational age, presence of pneumothorax, mode of delivery, single vs. multiple birth, and whether the birth occurred at Duke or at another hospital with later transfer to Duke. Tune the model, and compare its performance to that of a logistic regression model.\n",
    "\n",
    "The metadata for this dataset can be found [here](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/Cvlbw.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlbw = pd.read_csv('../data/vlbw.csv', index_col=0)\n",
    "vlbw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressors\n",
    "\n",
    "Another ensemble method for improving decision tree regression is to employ random forests, so that the\n",
    "effects of their over-fitting go away on average. \n",
    "\n",
    "Here we will use a random forest of 200 trees to reduce the tendency of each\n",
    "tree to over-fitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor(n_estimators=200, max_depth=9, \n",
    "                            min_samples_leaf=10)\n",
    "clf.fit(X, y)\n",
    "\n",
    "y_fit_200 = clf.predict(X_fit)\n",
    "\n",
    "plt.plot(X.ravel(), y, '.k', alpha=0.3)\n",
    "plt.plot(X_fit.ravel(), y_fit_200, color='red')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Selecting the optimal random forest regression model for the nashville daily temperature data via cross-validation in `scikit-learn`. Use the number of estimators and the maximim leaf nodes as tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References \n",
    "\n",
    "- T. Hastie, R. Tibshirani and J. Friedman. (2009) [Elements of Statistical Learning: Data Mining, Inference, and Prediction](http://statweb.stanford.edu/~tibs/ElemStatLearn/), second edition. Springer.\n",
    "\n",
    "- S. Marsland. (2009) [Machine Learning: An Algorithmic Perspective](Machine Learning: An Algorithmic Perspectivehttp://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html). CRC Press.\n",
    "\n",
    "- Data Robot Blog: [Gradient Boosted Regression Trees](http://www.datarobot.com/blog/gradient-boosted-regression-trees/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
