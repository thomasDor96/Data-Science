{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupvervised Learning\n",
    "\n",
    "Clustering is a class of unsupervised learning methods that associates observations according to some specified measure of **similarity** (e.g. Euclidean distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Algorithm\n",
    "\n",
    "The K-means clustering algorithm associates each point $x_i$ in a set of input points $\\{x_1, x_2, \\ldots, x_m\\}$ to $K$ clusters. Each cluster is specified by a **centroid** that is the average location of all the points in the cluster. The algorithm proceeds iteratively from arbitrary centroid locations, updating the membership of each point according to minimum distance, then updating the centroid location based on the new cluster membership. \n",
    "\n",
    "You might notice that this is just a special case of the **expectation maximization (EM)** algorithm. Recall that in EM we iteratively assigned labels to observations, according to which mixture component they were most likely to have been derived from. K-means is simpler, in that we just use the minimum distance to assign membership.\n",
    "\n",
    "The algorithm will have converged when the assignment of points to centroids does not change with each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "1. Initialize cluster centroids:\n",
    "\n",
    "$$\\mu^{(0)}_1, \\ldots, \\mu^{(0)}_k \\in \\mathbb{R}^n$$\n",
    "\n",
    "2. Iterate until converged:\n",
    "\n",
    "    a. Set $c_i = \\text{argmin}_j || x_i - \\mu_j^{(s)} ||$\n",
    "    \n",
    "    b. Update centroids:\n",
    "    \n",
    "    $$\\mu_j^{(s+1)} = \\frac{\\sum_{i=1}^m I[c_i = j] x_i}{\\sum_{i=1}^m I[c_i = j]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means algorithm is simply a Gaussian mixture model with two restrictions: \n",
    "\n",
    "1. the covariance matrix is spherical: \n",
    "\n",
    "    $$\\Sigma_k = \\sigma I_D$$\n",
    "\n",
    "2. the mixture weights are fixed:\n",
    "\n",
    "    $$\\pi_k = \\frac{1}{K}$$\n",
    "\n",
    "Hence, we are only interested in locating the appropriate centroid of the clusters. This serves to speed computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the distortion function:\n",
    "\n",
    "$$J(c,\\mu) = \\sum_{i]1}^m ||x_i - \\mu_{c_i}||$$\n",
    "\n",
    "which gets smaller at every iteration. So, k-means is coordinate ascent on $J(c,\\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $k$\n",
    "\n",
    "To check whether a chosen $k$ is reasonable, one approach is to compare the distances between the centroids to the mean distance bewween each data point and their assigned centroid. A good fit involves relatively large inter-centroid distances. \n",
    "\n",
    "The appropriate value for k (the number of clusters) may depend on the goals of the analysis, or it may be chosen algorithmically, using an optimization procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: clustering random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x, y = np.random.uniform(0, 10, 50).reshape(2, 25)\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with $k=4$, arbitrarily assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = (3, 3), (3, 7), (7, 3), (7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.scatter(*np.transpose(centroids), c='r', marker='+', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `cdist` from SciPy to calculate the distances from each point to each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "distances = cdist(centroids, list(zip(x,y)))\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the initial assignment to centroids by picking the minimum distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = distances.argmin(axis=0)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=np.array(list('rgbc'))[labels])\n",
    "plt.scatter(*np.transpose(centroids), c='r', marker='+', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can re-assign the centroid locations based on the means of the current members' locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centroids = [(x[labels==i].mean(), y[labels==i].mean())\n",
    "                 for i in range(len(centroids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=np.array(list('rgbc'))[labels])\n",
    "plt.scatter(*np.transpose(new_centroids), c='r', marker='+', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we simply iterate these steps until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = new_centroids\n",
    "iterations = 20\n",
    "\n",
    "for _ in range(iterations):\n",
    "    distances = cdist(centroids, list(zip(x,y)))\n",
    "    labels = distances.argmin(axis=0)\n",
    "    centroids = [(x[labels==i].mean(), y[labels==i].mean())\n",
    "                 for i in range(len(centroids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=np.array(list('rgbc'))[labels])\n",
    "plt.scatter(*np.transpose(centroids), c='r', marker='+', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Re-run the model using different initial centroid locations, and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-means using scikit-learn\n",
    "\n",
    "The `scikit-learn` package includes a `KMeans` class for flexibly fitting K-means models. It includes additional features, such as initialization options and the ability to set the convergence tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState(1)\n",
    "\n",
    "# Instantiate model\n",
    "kmeans = KMeans(n_clusters=4, random_state=rng)\n",
    "# Fit model\n",
    "kmeans.fit(np.transpose((x,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we can retrieve the labels and cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plot should look very similar to the one we fit by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=np.array(list('rgbc'))[kmeans.labels_])\n",
    "plt.scatter(*np.transpose(centroids), c='gray', marker='+', s=100)\n",
    "plt.scatter(*kmeans.cluster_centers_.T, c='r', marker='+', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Wine chemistry\n",
    "\n",
    "Recall the wine dataset in `wine.dat` that includes thirteen chemical measurements carried out on each of 178 wines from three regions of Italy. If we did not have the labels for the wines, we might be interested to see whether a clustering algorithm could correctly assign labels to the wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wine = pd.read_table(\"../data/wine.dat\", sep='\\s+')\n",
    "\n",
    "attributes = ['Grape',\n",
    "            'Alcohol',\n",
    "            'Malic acid',\n",
    "            'Ash',\n",
    "            'Alcalinity of ash',\n",
    "            'Magnesium',\n",
    "            'Total phenols',\n",
    "            'Flavanoids',\n",
    "            'Nonflavanoid phenols',\n",
    "            'Proanthocyanins',\n",
    "            'Color intensity',\n",
    "            'Hue',\n",
    "            'OD280/OD315 of diluted wines',\n",
    "            'Proline']\n",
    "\n",
    "wine.columns = attributes\n",
    "\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine.copy()\n",
    "y = X.pop('Grape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the analysis, and aid visualization, we will again perform a PCA to isolate the majority of the variation into two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True).fit(X)\n",
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine['First Component'] = X_pca[:, 0]\n",
    "wine['Second Component'] = X_pca[:, 1]\n",
    "\n",
    "sns.lmplot('First Component', 'Second Component', \n",
    "           data=wine, \n",
    "           fit_reg=False, \n",
    "           hue=\"Grape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a `KMeans` object with `k=3`, and fit the data with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_wine = KMeans(n_clusters=3, random_state=rng)\n",
    "km_wine.fit(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can extract the cluster centroids (in the `cluster_center_` attribute) and the group labels (in `labels_`) in order to generate a plot of the classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(km_wine.cluster_centers_, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_wine.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visually examine the clusters, and compare them to the known labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine['Cluster'] = km_wine.labels_\n",
    "wine.loc[km_wine.labels_==0, 'Cluster'] = 3\n",
    "\n",
    "grid = sns.lmplot('First Component', 'Second Component', \n",
    "           data=wine, \n",
    "           fit_reg=False, \n",
    "           hue=\"Cluster\")\n",
    "grid.ax.scatter(*wine.loc[wine.Grape!=wine.Cluster, ['First Component', 'Second Component']].values.T, \n",
    "             s=60, linewidth=1, facecolors='none', edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` includes a suite of well-known clustering algorithms. \n",
    "\n",
    "- `sklearn.cluster.Birch`\n",
    ": A memory-efficient, online-learning algorithm provided as an alternative to KMeans. It constructs a tree data structure with the cluster centroids being read off the leaf.\n",
    "- `sklearn.cluster.MeanShift`\n",
    ": Mean shift clustering aims to discover “blobs” in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids. But, mean shift is not scalable to high number of samples.\n",
    "- `sklearn.cluster.DBSCAN`\n",
    ": Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the number of clusters with silhouette analysis\n",
    "\n",
    "Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
    "\n",
    "**Silhouette coefficients** \n",
    "\n",
    "- values near +1 indicate that the sample is far away from the neighboring clusters. \n",
    "- value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters\n",
    "- values near -1 indicate that those samples might have been assigned to the wrong cluster\n",
    "\n",
    "Using the wine data as an example, the silhouette analysis can be used to choose an optimal value for n_clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X_pca) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = clusterer.fit_predict(X_pca)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X_pca, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X_pca[:, 0], X_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette plot shows that:\n",
    "\n",
    "- `n_clusters` value of 2 or 6 is a poor choice because there are negative coefficients\n",
    "- a value of 5 is probably a poor choice due to the wide range of silhouette plot sizes.\n",
    "\n",
    "One would probably choose the true value of 3, absent any labels, which is a good sign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: clustering baseball statistics\n",
    "\n",
    "We can use clustering to try to find interesting groupings among sets of baseball statistics. Load the baseball dataset and run a clustering algorithm on the set of three statistics:\n",
    "\n",
    "* hit rate: hits / at bats\n",
    "* strikeout rate: strikeouts / at bats\n",
    "* walk rate: bases on balls /at bats\n",
    "\n",
    "You should probably set a minimum number of at bats to qualify for the analysis, since there are pitchers that get only a handful of at bats each year.\n",
    "\n",
    "Since we are clustering in 3 dimensions, you can visualize the output as a series of pairwise plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseball = pd.read_csv(\"../data/baseball.csv\", index_col=0)\n",
    "baseball.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "Aside from issues with choosing the number of groups *a priori*, K-means has trouble identifying groups that are not spherical and convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "points, labels = datasets.make_moons(n_samples=100, noise=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = np.array(sns.color_palette(\"hls\", 8))\n",
    "\n",
    "plt.scatter(*points.T, color=palette[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moons = KMeans(n_clusters=2, random_state=rng)\n",
    "moons.fit(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*points.T, color=palette[moons.labels_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm, introduced [Ester et al. (1996)](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf).\n",
    "\n",
    "DBSCAN infers the number of clusters from the dataset, allowing it to discover clusters of arbitrary shape. It establishes a **neighborhood size**, and assigns points to categories based on their relationship to other points, conditioned on this neighborhood size. \n",
    "\n",
    "This confers several advantages:\n",
    "\n",
    "- Allows for more complex cluster shapes\n",
    "- K does not need to be specified\n",
    "- Automatically finds outliers\n",
    "\n",
    "While we don't need to choose K, we do need to select a distance function for quantifying *dissimilarity* between points.\n",
    "\n",
    "DBSCAN distinguishes between 3 different \"points\":\n",
    "\n",
    "- **Core points**: A core point is a point that has at least a minimum number of other points (MinPts) in its radius epsilon.\n",
    "- **Border points**: A border point is a point that is not a core point, since it doesn't have enough MinPts in its neighborhood, but lies within the radius epsilon of a core point.\n",
    "- **Noise points**: All other points that are neither core points nor border points.\n",
    "\n",
    "![](https://github.com/amueller/scipy-2017-sklearn/raw/e7d90ee4e382bebd44dfcea0042bca4698d8c57f/notebooks/figures/dbscan.png)\n",
    "\n",
    "The steps to the DBSCAN algorithm are:\n",
    "\n",
    "1. **Determine the type of a new point**\n",
    "\n",
    "    - core\n",
    "    - boundary\n",
    "    - outlier\n",
    "\n",
    "  We randomly pick that has not yet been assigned to a cluster, or been designated as an outlier. For this point, we determine its neighborhood. If it is a **core point**, we seed a cluster around it, otherwise label the point as an **outlier**.\n",
    "\n",
    "2. **Expand the new cluster by adding all reachable points**. Once we find a core point (and hence, a cluster), find all points that are reachable based in the neighborhood and add them to the cluster. If a point previously found to be an outlier is included, change its status to a **border point**.\n",
    "\n",
    "3. **Repeat** these steps until all points are either assigned to a cluster or designated as an outlier.\n",
    "\n",
    "The `DBSCAN` class in `scikit-learn` is a straightforward implementation of this algorithm, and requires three main arguments:\n",
    "\n",
    "- `eps` defines the size of the neighborhood around each point\n",
    "\n",
    "- `min_samples` is the number of points that needs to be within the neigborhood for a point to be considered a core point; density level threshold\n",
    "\n",
    "- `metric` is either a callable function or a string corresponding to a built-in distance metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan_moons = DBSCAN(eps=0.3,\n",
    "                      metric='euclidean')\n",
    "dbscan_moons.fit(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_moons.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*points.T, color=palette[dbscan_moons.labels_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "- Ester, M., H. P. Kriegel, J. Sander, and X. Xu (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231.\n",
    "- Raschka, S (2015) Python Machine Learning. Packt Publishing, Birmingham, UK."
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/ae9a27a5351908b9b1f611e14c0b91a6"
  },
  "gist": {
   "data": {
    "description": "Section6_2-Clustering.ipynb",
    "public": true
   },
   "id": "ae9a27a5351908b9b1f611e14c0b91a6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
