{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "In machine learning, we are often dealing with very large datasets, not only in terms of the number of rows, but also in the number of columns (*i.e.* features or predictors). This presents a challenge in choosing which variables ought to be included in a particular analysis. Inevitably, some features will be correlated with other features, implying that they are partially redundant in terms of explaining part of the variability in the outcome variable.\n",
    "\n",
    "To deal with this, we can apply one of several dimensionality reduction techniques, which aim to identify latent variables that are associated with both the features and the outcomes, but are complementary with one another in terms of the variability that they explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "The first **unsupervised learning** method that we will look at is Principal Component Analysis (PCA).\n",
    "It is a technique to reduce the dimensionality of the data, by creating a linear projection.\n",
    "That is, we find new features to represent the data that are a linear combination of the old data (i.e. we rotate it). Thus, we can think of PCA as a projection of our data onto a *new* feature space.\n",
    "\n",
    "The way PCA finds these new directions is by looking for the directions of maximum variance.\n",
    "Usually only few components that explain most of the variance in the data are kept. Here, the premise is to reduce the size (dimensionality) of a dataset while capturing most of its information. There are many reason why dimensionality reduction can be useful: It can reduce the computational cost when running learning algorithms, decrease the storage space, and may help with the so-called \"curse of dimensionality,\" which we will discuss in greater detail later.\n",
    "\n",
    "Here is an illustraion using the iris dataset we've seen previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris_df = (pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "               .assign(species=iris.target_names[iris.target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to visualize a 4-dimensional dataset simultaneously, but we can plot the data pairwise to get an idea of how the output (species labels) can be discriminated on the basis of each variable relative to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for xy in combinations(iris.feature_names, 2):\n",
    "    x, y = xy\n",
    "    sns.lmplot(x, y, \n",
    "           data=iris_df, \n",
    "           fit_reg=False, \n",
    "           hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, for example, that the petal variables appear to be redundant with respect to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What PCA will do is formulate a set of **orthogonal** varibles, where the number of orthogonal axes is smaller than the number of original variables. It then **projects** the original data onto these axes to obtain transformed variables. \n",
    "\n",
    "The key concept is that each set of axes constructed maximizes the amount of residual variability explained. \n",
    "\n",
    "We can then fit models to the subset of orthogonal variables that accounts for most of the variability.\n",
    "\n",
    "Let's do a PCA by hand first, before using scikit-learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "As we saw in the previous unit, an important first step for many datasets is to **standardize** the original data. Its important for all variables to be on the same scale because the algorithm will be seeking to maximize variance along each axis. If one variable is numerically larger than another variable, it will tend to have larger variance, and will therefore garner undue attention from the algorithm. \n",
    "\n",
    "This dataset is approximately on the same scale, though there are differences, particularly in the fourth variable (petal width):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a standardization transformation from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_std = StandardScaler().fit_transform(iris.data)\n",
    "X_std[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "\n",
    "The PCA algorithm is driven by the eigenvalues and eigenvectors of the original dataset. \n",
    "\n",
    "- The eigenvectors determine the direction of each component\n",
    "- The eigenvalues determine the length (magnitude) of the component\n",
    "\n",
    "The eigendecomposition is performed on the covariance matrix of the data, which we can derive here using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Σ = np.cov(X_std.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = np.linalg.eig(Σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "variables = [name[:name.find(' (')]for name in iris.feature_names]\n",
    "\n",
    "class Arrow3D(FancyArrowPatch):\n",
    "    def __init__(self, xs, ys, zs, *args, **kwargs):\n",
    "        FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\n",
    "        self._verts3d = xs, ys, zs\n",
    "\n",
    "    def draw(self, renderer):\n",
    "        xs3d, ys3d, zs3d = self._verts3d\n",
    "        xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\n",
    "        self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\n",
    "        FancyArrowPatch.draw(self, renderer)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(X_std[:,0], X_std[:,1], X_std[:,2], 'o', markersize=8, \n",
    "        color='green', \n",
    "        alpha=0.2)\n",
    "\n",
    "mean_x, mean_y, mean_z = X_std.mean(0)[:-1]\n",
    "ax.plot([mean_x], [mean_y], [mean_z], 'o', markersize=10, color='red', alpha=0.5)\n",
    "for v in evecs:\n",
    "    a = Arrow3D([mean_x, v[0]], [mean_y, v[1]], [mean_z, v[2]], mutation_scale=20, lw=3, arrowstyle=\"-|>\", color=\"r\")\n",
    "    ax.add_artist(a)\n",
    "ax.set_xlabel(variables[0])\n",
    "ax.set_ylabel(variables[1])\n",
    "ax.set_zlabel(variables[2])\n",
    "\n",
    "plt.title('Eigenvectors')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting components\n",
    "\n",
    "The eigenvectors are the principle components, which are normalized linear combinations of the original features. They are ordered, in terms of the amount of variation in the dataset that they account for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1)\n",
    "\n",
    "total = evals.sum()\n",
    "variance_explained = 100* np.sort(evals)[::-1]/total\n",
    "\n",
    "axes[0].bar(range(4), variance_explained)\n",
    "axes[0].set_xticks(range(4));\n",
    "axes[0].set_xticklabels(['Component ' + str(i+1) for i in range(4)])\n",
    "\n",
    "axes[1].plot(range(5), np.r_[0, variance_explained.cumsum()])\n",
    "axes[1].set_xticks(range(5));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting the data\n",
    "\n",
    "The next step is to **project** the original data onto the orthogonal axes.\n",
    "\n",
    "Let's extract the first two eigenvectors and use them as the projection matrix for the original (standardized) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = evecs[:, :2]\n",
    "Y = X_std @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj = pd.DataFrame(np.hstack((Y, iris.target.astype(int).reshape(-1, 1))),\n",
    "                      columns=['Component 1', 'Component 2', 'Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('Component 1', 'Component 2',\n",
    "          data=df_proj,\n",
    "          fit_reg=False,\n",
    "          hue='Species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA in scikit-learn\n",
    "\n",
    "`scikit-learn` provides a PCA transformation in its `decomposition` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3, whiten=True).fit(iris.data)\n",
    "X_pca = pca.transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['First Component'] = X_pca[:, 0]\n",
    "iris_df['Second Component'] = X_pca[:, 1]\n",
    "iris_df['Third Component'] = X_pca[:, 2]\n",
    "\n",
    "sns.lmplot('First Component', 'Second Component', \n",
    "           data=iris_df, \n",
    "           fit_reg=False, \n",
    "           hue=\"species\");\n",
    "\n",
    "sns.lmplot('Second Component', 'Third Component',\n",
    "           data=iris_df, \n",
    "           fit_reg=False, \n",
    "           hue=\"species\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Import the wine dataset and perform PCA on the predictor variables, and decide how many principal components would you select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_table('../data/wine.dat', sep='\\s+')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
