{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "\n",
    "## Week 2. Machine Learning in Python\n",
    "\n",
    "### Objectives    \n",
    "\n",
    "- Introduction to scikit-learn \n",
    "- Implementation of machine learning algorithms in scikit-learn  \n",
    "    - Explore a dataset and prepare it for `sklearn`. \n",
    "    - Build a predictive `DecisionTreeClassifier` model for the Titanic Survival Dataset.  \n",
    "    - Evaluate the model using different metrics. \n",
    "    \n",
    "### Prerequisites   \n",
    "\n",
    " - You should have the following python packages installed:\n",
    "    - [numpy](http://www.scipy.org/scipylib/download.html)\n",
    "    - [pandas](http://pandas.pydata.org/getpandas.html)\n",
    "    - [matplotlib](http://matplotlib.org/index.html)\n",
    "    - [seaborn](http://seaborn.pydata.org) \n",
    "    - [sklearn](http://scikit-learn.org/stable/install.html)\n",
    " - If you're rusty on basic python programming and exploratory data analysis, check out Jupyter notebooks from week 1 in the [Udacity_connect repo](https://github.com/yanfei-wu/Udacity_connect/tree/master/wk1).  \n",
    " \n",
    "*Acknowledgement: Part of this notebook is adapted from Nick Hoh's [repo](https://github.com/nickypie/ConnectIntensive/blob/master/lesson-03-part-01.ipynb).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 | Introduction to Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is scikit-learn?  \n",
    "\n",
    "Scikit-learn (`sklearn`) is an open-source machine learning library for Python. It provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python.  \n",
    "\n",
    "<img src=\"sklearn.png\" width=\"600\"/>\n",
    "\n",
    "Scikit-learn is built upon Numpy and [SciPy](https://www.scipy.org) (Scientific Python) and complements this scientific environment with machine learning algorithms.  \n",
    "\n",
    "\n",
    "\n",
    "### Algorithms \n",
    "\n",
    "- Supervised learning:  \n",
    "    - Linear models (Ridge, Lasso, Elastic Net, ...)  \n",
    "    - Support Vector Machines  \n",
    "    - Tree-based methods (Random Forests, Bagging, GBRT, ...)  \n",
    "    - Nearest neighbors  \n",
    "    - Neural networks  \n",
    "    - Gaussian Processes  \n",
    "    - Feature selection   \n",
    "    \n",
    "- Unsupervised learning:  \n",
    "    - Clustering (KMeans, Ward, ...)    \n",
    "    - Matrix decomposition (PCA, ICA, ...)  \n",
    "    - Density estimation  \n",
    "    - Outlier detection  \n",
    "\n",
    "- Model selection and evaluation:  \n",
    "    - Cross-validation  \n",
    "    - Grid-search  \n",
    "    - Lots of metrics  \n",
    "\n",
    "... and many more! (See our [Reference](http://scikit-learn.org/stable/))  \n",
    "\n",
    "\n",
    "### Installation \n",
    "\n",
    "Scikit-learn requires:  \n",
    "- Python (>= 2.7 or >= 3.3)    \n",
    "- NumPy (>= 1.8.2)    \n",
    "- SciPy (>= 0.13.3)    \n",
    "\n",
    "The easiest way is to install is via `pip` or `conda`:  \n",
    "\n",
    "```sh\n",
    "pip install -U scikit-learn\n",
    "conda install scikit-learn\n",
    "```  \n",
    "\n",
    "### Resources  \n",
    "\n",
    "- [Documentation and examples](http://scikit-learn.org/stable/index.html) from scikit-learn homepage  \n",
    "- Scikit-learn's [Github Repository](https://github.com/scikit-learn) for the code  \n",
    "- Books:  \n",
    "    - [Building Machine Learning Systems with Python](https://www.amazon.com/dp/1782161406?tag=inspiredalgor-20)  \n",
    "    - [Learning scikit-learn: Machine Learning in Python](https://www.amazon.com/dp/1783281936?tag=inspiredalgor-20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> *Note: the scikit-learn is focused on modeling data, not on loading and manipulating data. For these features, refer to NumPy and Pandas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 | Step-by-Step Guide of Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries   \n",
    "\n",
    "As usual, we start by importing some useful libraries and modules. Make sure you have `scikit-learn` installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    print(\"Successfully imported sklearn! (Version {})\".format(sklearn.__version__))\n",
    "    skversion = int(sklearn.__version__[2:4])\n",
    "except ImportError:\n",
    "    print(\"Could not import sklearn!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set maximum rows to display\n",
    "pd.options.display.max_rows = 15 # default is 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Reading in the data & Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we are going to use the famous Titanic Survival dataset from [Kaggle](https://www.kaggle.com/c/titanic) (only the training set will be used; a little preprocessing of the dataset was done). This is like the \"Hello World!\" in machine learning. The goal of this notebook is to demonstrate how we can leverage the predictive models from scikit-learn (sklearn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data into pandas dataframe\n",
    "data = pd.read_csv('./data/titanic_data.csv')\n",
    "print 'Titanic dataset loaded!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some general infromation of the dataset \n",
    "# view the first 5 rows of the dataframe \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a concise summary of data \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above information, we know that the dataset contains **889 rows and 12 columns**. There are **both numerical variables and categorical variables** in the dataset. Also notice that some columns contain **missing values**, for features `Age` and `Cabin`.   \n",
    "\n",
    "The key feature we will attempt to predict (i.e., the target variable) in this project is the `Survived` feature, which is equal to 0 or 1 for a passenger who died or survived, respectively. Note that you should start any machine learning project with exploratory data analysis to learn more about the dataset.  \n",
    "\n",
    "> *Note: If you're already comfortable with exploratory data analysis in pandas, and want to move on to prediction with the sklearn library, feel free to skip ahead.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISES - Data Exploration & Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: How many people survived? How many people did not survive? Answer with both numbers and visualizaton.   \n",
    "**Hint:** `seaborn.countplot()` could be a handy tool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: How many males and females are in the dataset? Plot barplot showing suvived/deceased by `Sex`.  \n",
    "**Hint:** look up the `hue` parameter in `seaborn.countplot()`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: How do the counts of suvived passengers differ among different `Pclass`?  \n",
    "**Hint:** Again, `seaborn.countplot()` can be a handy visualization tool. To calculate the exact count, look into [`pandas.groupby()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: From which port did the most passengers embark? Which port with the fewest passengers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: How is the age distribution like in the Titanic passengers? Plot a histogram and also provide some summary statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6: How is age distribution different among passengers from different `Pclass`? \n",
    "**Hint:** A side-by-side boxplot or violin plot can be useful for this question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7: Is there something else in the dataset you think worth exploring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Preprocessing and cleaning the data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are approaching a machine learning problem, we should always start with a well-posed problem in mind. The question here in this project could be \"Can we predict whether or not a passenger aboard the Titanic survived using the other features present in the Titanic data set?\" Then, based on our exploratory data analysis, we can build a predictive model using features we select from the original dataset. For example, we can use:  \n",
    "\n",
    "- `Pclass`: Passenger's class (1 = first; 2 = second; 3 = third)\n",
    "- `Sex`: Passenger's sex (male or female)\n",
    "- `Age`: Passenger's age\n",
    "- `SibSp`: Number of passenger's siblings/spouses on board\n",
    "- `Parch`: Number of passenger's parents/children on board\n",
    "- `Fare`: Price of the passenger's ticket\n",
    "- `Embarked`: The port of embarkation (`C` = Cherbourg, `Q` = Queenstown, `S` = Southampton) \n",
    "\n",
    "Of course, sometimes we can create more features by manipulating the original features. For example, in this dataset, with the passengers' names, we can extract the prefix `Mr`, `Mrs`, and `Miss` as a possible feature. Or we can compute the total number of family members on board for a passenger by summing `SibSp` and `Parch`. \n",
    "\n",
    "Most of the times, we will have to do some preprocessing for the dataset to be ready for model building with `sklearn`.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Missing Values \n",
    "\n",
    "As shown by `data.info()` above, we have some missing values in our dataset. In addition to relying on `.info()` to check if there are missing values. We can also use `pandas.isnull()` or even visualize the distribution of missing values with `seaborn`. **Run** the following cells to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.isnull() \n",
    "pd.isnull(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the table we obtained above is actually less useful\n",
    "# we want the total number of missing values by column\n",
    "pd.isnull(data).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use seaborn to visualize missing values\n",
    "sns.heatmap(pd.isnull(data), yticklabels=False, cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly 20 percent of the `Age` data is missing. The proportion of `Age` missing is likely small enough for reasonable replacement with some form of imputation. For the `Cabin` column, it seems that we are missing too much data to do something useful with the data as it is. We can either drop this feature, or change it to another feature, e.g., \"Cabin Known: 1 or 0\".   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Discussion:** if we would like to use `Age` as one our features but 20% of them are missing, what can we do? Propose some options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISES  \n",
    "\n",
    "#### Q1: Implement your way of dealing with the missing `Age` values in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Categorical Variable - `pandas.Series.map()` and `pandas.get_dummies()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data.info()` we ran above also shows that there are both numerical variables and categorical variables in the dataset. Note that for categorical variables, we need to convert them to numerical variables.\n",
    "\n",
    "The method [`pandas.Series.map()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) can take a Python dictionary as a parameter or argument. The keys of the dictionary are the current entries in the `Series` object, while the values of the dictionary are the new desired entries for the `Series` object. For example, consider the `'Sex'` feature. We'd like to map the two genders, `'female'` and `'male'`, to numbers, e.g. 0 and 1.  \n",
    "\n",
    "**Run** the cell below to use `Series.map()` to map the genders `'female'` and `'male'` to 0 and 1, respectively, then display the first few rows of the `DataFrame` object, and the `dtype`, to show that the `'Sex'` feature is now numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Sex'] = data['Sex'].map({'female': 0, 'male': 1, 0:0, 1:1})\n",
    "display(data.head())\n",
    "display(data['Sex'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about other categorical variables, e.g., `'Embarked'` feature? We could map the `'Embarked'` feature to numerical values, as in the dictionary `{'C': 0, 'Q': 1, 'S': 2}`. \n",
    "\n",
    "However, such integer representation would interpret the categories as being ordered, i.e., Cherbourg < Queenstown < Southampton, which is clearly not the case. Therefore, we can use the so-called [one-hot encoding](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science) to turn the features into dummy variables, which can be implemented by [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html). \n",
    "\n",
    "**Run** the cell below to see what the dummy variables for the `Series` object `data['Embarked']` look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embarked_dummies = pd.get_dummies(data['Embarked'])\n",
    "display(embarked_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original categorical variable `Embarked` had three distinct categories: `C` for Cherbourg, `Q` for Queenstown, and `S` for Southampton. By calling `pandas.get_dummies(data['Embarked'])`, we get a `DataFrame` object where the number of columns corresponds to the number of distinct categories in the `Series` object. Each column is called a **dummy variable** or an **indicator variable**. The names of the dummy variables are simply the categories from the original `Series` object. The values of the dummy variable `C` are 1 in the rows where `'Embarked'` is equal to `C`, and 0 otherwise.\n",
    "\n",
    "But the new column names `C`, `Q`, `S` are less descriptive than the original `Embarked`. We could specify the `prefix` parameter in `pd.get_dummies()` as `Embarked` so that the three new column names make more sense. \n",
    "\n",
    "**Run** the cell below to see how it looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embarked_dummies = pd.get_dummies(data['Embarked'], prefix='Embarked')\n",
    "display(embarked_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Final Dataset Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop columns will not be used\n",
    "data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True) # note inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate the dummy variables created by `Embarked` to the \n",
    "data = pd.concat([data, embarked_dummies], axis=1)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the preprocessed data  \n",
    "\n",
    "Now that we've preprocessed and cleaned the training and testing datasets, we want to save our progress so that we don't have to repeat preprocessing. **Run** the cell below to write DataFrame object to a csv file  using [`pandas.DataFrame.to_csv()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('./data/titanic_cleaned.csv', index=False)\n",
    "print 'Cleaned Titanic training data set saved!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Choosing the right model and learning algorithm  \n",
    "\n",
    "With the problem we formulated for this project and our knowledge about the dataset, we know that we are dealing with a supervised classification problem. Choosing the right model and learning algorithm is a big topic in machine learning and we will focus on it in our future sessions. For now, let's just start with one of the simplest model for this problem, `DecisionTreeClassifier` from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import \n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by splitting our data. As covered in the lecture, we can split the data into training, validation, and test sets. \n",
    "  - **Training set:** A set of examples used training, i.e., to fit the parameters of model.\n",
    "  - **Validation set:** A set of examples used to tune the model.\n",
    "  - **Test set:** A set of examples used only *once* to see how the model generalizes to unseen data.\n",
    "  \n",
    "#### `train_test_split()` \n",
    "\n",
    "The latest version of the library `sklearn` has the module `model_selection`, which contains the method [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). We can use this method to split `data` further into a training and a validation set. The arguments that we need to pass to `train_test_split()` are: \n",
    "\n",
    "  - `X` and `y`: Arrays. These can be `pandas` `DataFrame` or `Series` objects. \n",
    "  - `test_size`: A float with the proportion of data to put into the test set, e.g., `test_size = 0.1` would 10% of the data as the test set.\n",
    "  - `random_state`: The pseudo-random number generator state used for random sampling. For a given value of `random_state`, the method will partition the data set exactly the same way each time, which is useful for debugging. \n",
    "  - `stratified`: \n",
    "\n",
    "**Run** the cell below to create the `DataFrame` object `X` and the `Series` object `y` from `data`. Then use `train_test_split()` with a `random_state` to split the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "# X is a pandas DataFrame object with the features\n",
    "X = data.drop('Survived', axis=1)\n",
    "\n",
    "# y is a pandas Series object with the target variable 'Survived'\n",
    "y = data['Survived'] \n",
    "\n",
    "# Split the data into training and test data sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "# Split the training data into training and validation sets: \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=21)\n",
    "\n",
    "# Take a look at the first few rows of the training features and classes\n",
    "display(X_train.head())\n",
    "display(y_train.head())\n",
    "\n",
    "# Verify that the data sets were split 80% training and 20% testing\n",
    "print 'The number of instances in the original data: {}'.format(data.shape[0])\n",
    "print 'The number of instances in the training set: {}'.format(len(y_train))\n",
    "print 'The number of instances in the validation set: {}'.format(len(y_valid))\n",
    "print 'The number of instances in the test set: {}'.format(len(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Classifier\n",
    "\n",
    "For supervised learning problems, the model building `sklearn` workflow is pretty similar, regardless of the type of classifier you'd like to build: \n",
    "  1. **Create** a classifier object. \n",
    "  2. **Train** the classifier on the training data set. \n",
    "  3. **Predict** with the classifier on the validation (test) data set. \n",
    "  4. **Assess** the performance of the classifier by comparing the predictions to the actual labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier and accuracy_score from the appropriate sklearn modules\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. CREATE the classifier object... in this example, we call it clf1\n",
    "clf1 = DecisionTreeClassifier(random_state=21)\n",
    "\n",
    "# 2. TRAIN the classifier object using the method .fit()\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "# 3. PREDICT labels for the validation (test) set using the method .predict()\n",
    "y_pred_train = clf1.predict(X_train)\n",
    "y_pred_valid  = clf1.predict(X_valid)\n",
    "\n",
    "# 4. ASSESS the accuracy of the classifier, comparing the predictions to the actual labels.\n",
    "print \"The model has an accuracy of {:.1f}% on the training set, and {:.1f}% on the validation set\".\\\n",
    "      format(100.0*accuracy_score(y_train, y_pred_train),\\\n",
    "             100.0*accuracy_score(y_valid, y_pred_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Do you think the above model might be overfitting or underfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: Check what are available parameters in `DecisionTreeClassifier`. Play with different parameters and check how accuracy is different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can do something better than using accuracy to evaluate our model. We have covered a number of performance metrics for classification model in the lecture, such as confusion matrix, precision and recall, F-beta score. In the following exercise, you will calculate the metrics for the model you just built above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Show a confusion matrix for the validation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: What are the precision and recall for your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: Calculate the training and test F1 scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Do Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how comfortable you are about the content covered in this tutorial, you can \n",
    "\n",
    "- Read scikit-learn documentation and find out what other classification models are available. Pick one of them and build a classification model for this problem. Evaluate the model you pick and compare it with `DecisionTreeClassifier`.  \n",
    "\n",
    "- Implemented multiple classification models and use cross validation to pick the best one. Try improve the performance of the model you pick.  \n",
    "\n",
    "- Choose a classification dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html) and do step 1 through step 5 for the dataset you pick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mlnd]",
   "language": "python",
   "name": "conda-env-mlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
