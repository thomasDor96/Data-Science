{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "\n",
    "## Week 7. PCA Miniproject\n",
    "\n",
    "### Objectives    \n",
    "\n",
    "  - Perform Principal Component Analysis (PCA) on a large set of features to explain as much of the variance as possible in the data using a smaller set of features.\n",
    "  - Recognize differences between `train_test_split()` and `StratifiedShuffleSplit()` when creating training and testing sets.\n",
    "  - Introduce the `class_weight` parameter for `SVC()`, to see how correctly predicting targets from a smaller class size can be weighted more heavily.\n",
    "  - Visualize the eigenfaces (orthonormal basis of components) that result from PCA.\n",
    "  \n",
    "### Prerequisites\n",
    "  - [matplotlib](http://matplotlib.org/index.html)  \n",
    "  - [numpy](http://www.scipy.org/scipylib/download.html)  \n",
    "  - [pandas](http://pandas.pydata.org/getpandas.html)  \n",
    "  - [sklearn](http://scikit-learn.org/stable/install.html)  \n",
    "  \n",
    "> This notebook builds upon [`pca/eigenfaces.py`](https://github.com/udacity/ud120-projects/blob/master/pca/eigenfaces.py) from [**Udacity ud120-projects repo**](https://github.com/udacity/ud120-projects). It derives from [an eigenfaces/SVM example in the sklearn documentation](http://scikit-learn.org/0.18/auto_examples/applications/face_recognition.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 | Get the Data\n",
    "\n",
    "The dataset for this mini-project comes from [\"Labeled Faces in the Wild\" (LFW)](http://vis-www.cs.umass.edu/lfw/), a database of more than 13,000 face photographs designed for studying the problem of unconstrained face recognition. We are going to use the **funneled** dataset, which comes from the following paper:\n",
    "\n",
    "> Gary B. Huang, Vidit Jain, and Erik Learned-Miller. [**Unsupervised Joint Alignment of Complex Images**](http://vis-www.cs.umass.edu/papers/iccv07alignment.pdf). *International Conference on Computer Vision (ICCV), 2007.*\n",
    "\n",
    "Section 2 of the paper describes the preprocessing steps of [congealing](http://vis-www.cs.umass.edu/congeal.html) and funneling to align images in the dataset.\n",
    "\n",
    "The original source of this PCA demo comes from the [scikit-learn documentation](http://scikit-learn.org/0.18/auto_examples/applications/face_recognition.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Display progress logs on stdout\n",
    "# useful to monitor progress while downloading LFW data\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run** the cell below to download the data using [the `sklearn` function `fetch_lfw_people()`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html). The data is ~200MB, so the download may take a while. The data will be loaded into the dictionary `lfw_people`. The dictionary contains several items:\n",
    "  - `lfw_people.DESCR`: string -- a short description of the data dictionary.\n",
    "  - `lfw_people.data`: 2-D numpy array (`dtype=float32`) with shape `(n_samples, n_features)`\n",
    "    - Each entry in the array is a value ranging from 0.0 to 255.0, denoting the 8-bit [grayscale](https://en.wikipedia.org/wiki/Grayscale) value of each pixel.\n",
    "    - `n_samples`: the total number of images, the instances/inputs in the dataset.\n",
    "    - `n_features`: the number of pixels per image, the features of the dataset.\n",
    "  - `lfw_people.images`: 3-D numpy array (`dtype=float32`) with shape `(n_samples, height, width)`.\n",
    "    - `images` is just `data` reshaped, so that `height` $\\times$ `width` = `n_features`\n",
    "  - `lfw_people.target`: 1-D numpy array (`dtype=int64`) of length `n_samples`, the labels of each image\n",
    "  - `lfw_people.target_names`: 1-D numpy array of length `n_classes`\n",
    "  \n",
    ">**Note:** Sometimes [the pillow module](https://python-pillow.org/) (which is being used in this example) can cause trouble with this notebook. If you get an error related to the `fetch_lfw_people()` command, try the following:\n",
    "\n",
    ">``pip install --upgrade PILLOW``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the data, if not already on disk, and load it as numpy arrays\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or you can use the pickled file included in the folder with this notebook\n",
    "'''import pickle\n",
    "with open('lfw_people.pickle', 'rb') as handle:\n",
    "    lfw_people2 = pickle.load(handle)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** In the call to `fetch_lfw_people()`, the parameter `min_faces_per_person` is set to 70. This ensures that the extracted dataset will only retain pictures of people that have at least 70 different pictures. The `resize` parameter of 0.4 scales each image to 40% of its original size, so that the dataset takes up less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the indices and names of the people in the lfw_people dataset\n",
    "for idx, name in enumerate(lfw_people.target_names):\n",
    "    print \"{}: {}\".format(idx, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run** the cell below to extract the shape information (number of samples, number of features, image height and width) from `lfw_people.images` and `lfw_people.data`. You will see a summary of the dataset: the numbers of samples (images), features (pixels), and classes (targets). You will also see the number of images per target in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w = lfw_people.images.shape\n",
    "\n",
    "# for machine learning we use the 2-D data directly\n",
    "# relative pixel positions info will be ignored by this model\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the target ID of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "# print a summary of the total dataset size\n",
    "print 'Total dataset size:'\n",
    "print '  n_samples  : {:>4}'.format(n_samples)\n",
    "print '  n_features : {:>4}'.format(n_features)\n",
    "print '  n_classes  : {:>4}\\n'.format(n_classes)\n",
    "\n",
    "# print the number of images for each target in the dataset\n",
    "print 'Images per Target:'\n",
    "for idx, name in enumerate(lfw_people.target_names):\n",
    "    print '  {:<18}: {:>3} images'.format(name, np.sum(lfw_people.target == idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 | Split the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run** the cells below to split the data into training and test (validation) sets using [`train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). As a comparison, we also split the data using `StratifiedShuffleSplit()` and check the percentages of each target in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into a training and testing set using train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Summarize the proportions of each label in the training and testing sets\n",
    "print 'train_test_split() results:'\n",
    "print '{:>28}{:>8}'.format('train', 'test')\n",
    "for idx, name in enumerate(lfw_people.target_names):\n",
    "    print '  {:<18}: {:6.1f}% {:6.1f}%'.format(name,\\\n",
    "                  np.sum(y_train == idx) * 100.0 / len(y_train),\\\n",
    "                  np.sum(y_test == idx) * 100.0 / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with StratifiedShuffleSplit()\n",
    "sss = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train_s, X_test_s = X[train_index], X[test_index]\n",
    "    y_train_s, y_test_s = y[train_index], y[test_index]\n",
    "\n",
    "# Summarize the proportions of each label in the training and testing sets\n",
    "print '\\nStratifiedShuffleSplit() comparison:'\n",
    "print '{:>28}{:>8}'.format('train', 'test')\n",
    "for idx, name in enumerate(lfw_people.target_names):\n",
    "    print '  {:<18}: {:6.1f}% {:6.1f}%'.format(name,\\\n",
    "                  np.sum(y_train_s == idx) * 100.0 / len(y_train_s),\\\n",
    "                  np.sum(y_test_s == idx) * 100.0 / len(y_test_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION: \n",
    "What is the difference between `train_test_split()` and `StratifiedShuffleSplit()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 | Principal Component Analysis\n",
    "\n",
    "The goal of [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) is to describe the maximal amount of variance in a dataset by the fewest number of composite features. \n",
    "\n",
    "**Run** the cell below to perform [Principal Component Analysis with `sklearn`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) on the training set of images to find the *eigenfaces*, or the orthonormal basis resulting from PCA.\n",
    "\n",
    ">**Note:** There's a little background on [Singular Value Decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) and [Randomized SVD](https://research.facebook.com/blog/fast-randomized-svd/) that may be useful or interesting to read here... but it's not necessary to fully understand SVD to appreciate the results of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a PCA (eigenfaces) on the face dataset\n",
    "\n",
    "# number of components\n",
    "n_components = 150\n",
    "\n",
    "# perform PCA on the training set\n",
    "print 'Extracting the top {} eigenfaces from {} faces'.format(n_components, X_train.shape[0])\n",
    "t0 = time() # track time\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True).fit(X_train)\n",
    "print 'done in {:0.3f}s'.format(time() - t0)\n",
    "\n",
    "# Reshape the PCA components based on the image dimensions\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "# Project input data \n",
    "print 'Projecting the input data on the eigenfaces orthonormal basis'\n",
    "t0 = time()\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print 'done in {:0.3f}s'.format(time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've conducted PCA on the training set, we can see how much of the variance in the dataset is explained by each eigenface (or each component of the orthonormal basis). The components are ordered by ratio of explained variance in descending order, and after conducting PCA this information is stored in the attribute `pca.explained_variance_ratio_`. **Run** the cell below to print the explained variance percentage from the first five eigenfaces, and the total explained variance percentage of all 150 eigenfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the explained variance\n",
    "for idx, var in enumerate(pca.explained_variance_ratio_[:5]):\n",
    "    print 'Eigenface {} explains {:5.2f}% of the variance.'.format(idx+1, var*100.0)\n",
    "    \n",
    "print '\\nIn total, the first {} eigenfaces explain {:5.2f}% of the variance.'\\\n",
    "      .format(len(pca.explained_variance_ratio_),\\\n",
    "              100.0*np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 150 components account for over 90% of the variance!. This leads us to believe that using these 150 components, we would recover most of the essential characteristics of the data. We can also plot the cumulative explained variance as a function of n_components. **Run** the cell below to find out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cumulative explained variance as a function of n_components\n",
    "plt.figure(figsize=(8, 6))\n",
    "var1 = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100) # Cumulative Variance explains\n",
    "plt.plot(var1, color='b')\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Cumulative explained variance (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a Support Vector Machine classifier. As we can see, the dataset has class imbalance problem. One way of dealing with class imbalance is to adjust the `class_weight` parameter in `SVC()`, so that the model score improves by different amounts depending on the class of the that targets are accurately classified. For a more detailed explanation of the `class_weight` parameter, you can read the [`SVC()` documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), or check out this [stackoverflow post](http://stackoverflow.com/questions/30972029/how-does-the-class-weight-parameter-in-scikit-learn-work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = n_samples*1.0 / (n_classes * np.bincount(y_train))\n",
    "print '{:<23}{:<8}{:<6}'.format('Target Name', 'Weight', 'Count')\n",
    "for idx, name in enumerate(lfw_people.target_names):\n",
    "    print '  {:<18}: {:6.2f} {:>6}'.format(name, weights[idx], np.sum(y_train == idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above, rarer classes or targets are weighted more heavily, i.e., it's more important for the classifier to get those targets correct. Now that we understand how the `class_weight` parameter adds weight to the rarer classes, we can use [`GridSearchCV()`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to find the optimal parameters for a SVM classifier. Here, we're using the radial basis function (RBF) kernel, and we're tuning the parameters `C` (\"hard-margin\" SVM when `C` is large vs. \"soft-margin\" SVM when `C` is small) and `gamma` (data points have a long-ranged influence when `gamma` is small, or a short-ranged influence when `gamma` is large). \n",
    "\n",
    "**Run** the cell below to find the optimal values for the `C` and `gamma` parameters in the `SVC()` classifier,  using the eigenfaces (`X_train_pca`) as the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a SVM classification model\n",
    "print 'Fitting the classifier to the training set'\n",
    "t0 = time()\n",
    "param_grid = {'C': [1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 5e3, 1e4],\n",
    "              'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "clf = clf.fit(X_train_pca, y_train)\n",
    "\n",
    "# Display the detailed results of the grid search + cross-validation:\n",
    "print 'done in {:0.3f}s'.format(time() - t0)\n",
    "print 'Best parameters set found on development set:\\n'\n",
    "print clf.best_params_\n",
    "print '\\nGrid scores:\\n'\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print '%0.3f (+/-%0.03f) for %r' % (mean, std * 2, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run** the cell below to make predictions on the test data and use a classification report and a confusion matrix to summarize the classifier performance. \n",
    "\n",
    ">**Note:** We trained the classifier using the *projections* of the training set features onto the *eigenfaces* (the orthonormal basis of transformed coordinates), so we must also make predictions using the projections of the test set features onto eigenfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Predicting the people names on the testing set'\n",
    "t0 = time()\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print 'done in {:0.3f}s'.format(time() - t0)\n",
    "\n",
    "print classification_report(y_test, y_pred, target_names=target_names)\n",
    "print confusion_matrix(y_test, y_pred, labels=range(n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a qualitative evaluation of the predictions using matplotlib. **Run** the cell below to define `plot_gallery()` and `title()` helper functions, and then visualize the first few faces from the test set, labeled with the predicted and actual classes. How did our optimal classifier do on these faces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
    "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i], size=12)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "\n",
    "# plot the result of the prediction on a portion of the test set\n",
    "def title(y_pred, y_test, target_names, i):\n",
    "    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n",
    "    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n",
    "    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n",
    "\n",
    "prediction_titles = [title(y_pred, y_test, target_names, i)\n",
    "                     for i in range(y_pred.shape[0])]\n",
    "\n",
    "plot_gallery(X_test, prediction_titles, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the gallery of the most significative eigenfaces\n",
    "eigenface_titles = ['eigenface %d' % i for i in range(eigenfaces.shape[0])]\n",
    "plot_gallery(eigenfaces, eigenface_titles, h, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very interesting, and give us insight into how the images vary: for example, the first few eigenfaces seem to be associated with the angle of lighting on the face, and later principal vectors seem to be picking out certain features, such as eyes, noses, and lips. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** Change n_components to the following values: [10, 15, 25, 50, 100, 250]. For each number of principal components, note the F1 score for Ariel Sharon. If you see a higher F1 score, does it mean the classifier is doing better, or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Do you see any evidence of overfitting when using a large number of PCs? Does the dimensionality reduction of PCA seem to be helping your performance here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mlnd]",
   "language": "python",
   "name": "conda-env-mlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
