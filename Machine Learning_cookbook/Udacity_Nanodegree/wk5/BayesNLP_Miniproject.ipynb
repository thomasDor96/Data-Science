{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Connect Intensive - Machine Learning Nanodegree\n",
    "\n",
    "## Week 5. Bayes NLP Miniproject\n",
    "\n",
    "### Objectives    \n",
    "\n",
    "  - Understand [Bayes Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) and [conditional probability](https://en.wikipedia.org/wiki/Conditional_probability)\n",
    "  - Write methods, utilizing Python dictionary objects and string methods such as `str.split()`.\n",
    "  - Apply Bayes Theorem to simple NLP: missing word prediction problems\n",
    "  \n",
    "### Prerequisites\n",
    "  - Basic Python knowledge in strings and dictionaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I | Understand Bayes Theorem \n",
    "\n",
    "Here is a brief description of Bayes theorem.\n",
    "\n",
    "Bayesian learning starts from an application of conditional probability. Suppose we have some **hypothesis** $h$ that occurs with probability $P(h)$. For example, in cancer diagoses, one hypothesis could be $h = $ \"patient has cancer\". We may call the set of all possible hypotheses $H$, or the **hypothesis space**. If we have no other data about the patient, $P(h)$ is known as the **prior probability**, i.e., the probability of the patient having cancer prior to getting any test results from the patient.\n",
    "\n",
    "Now suppose there's some diagnostic test we can conduct for the specific type of cancer. The test can come back positive or negative. We can represent this fact as the **training data** $D$ for the instance. For example, one possible training data could be $D = $ \"test result for the patient is negative\". We can then write a **conditional probability** $P(h|D)$ (\"probability of hypothesis $h$ given training data $D$\"). For our example, this represents the probability that the patient has cancer, given that the test result is negative. Because we evaluate this probability *after* knowing the training data, this quantity is also known as the **posterior probability**.\n",
    "\n",
    "The probability of the **conjunction** $\\land$ of two events can be computed by conditional probabilities:\n",
    "$$P(D \\land h) = P(D|h)\\cdot P(h) = P(h|D)\\cdot P(D)$$\n",
    "Here, the quantity $P(D \\land h)$ represents the probability of the training data $D$ and the hypothesis $h$ **both** being true for our patient.\n",
    "\n",
    "$P(D|h)\\cdot P(h)$ is the product of the **conditional probability of training data $D$ given the hypothesis $h$ is true (likelihood)**, and the **prior probability of hypothesis $h$ being true**. Here, we've conditioned on $h$ being true.\n",
    "\n",
    "$P(h|D)\\cdot P(D)$ is the product of the **conditional (posterior) probability of hypothesis $h$ given the training data $D$ is true**, and the **marginal likelihood of the training data $D$ being true under all possible hypothese**. Here, we've conditioned on $D$ being true.\n",
    "\n",
    "Bayes theorem solves the above equation for the posterior probability $P(h|D)$:\n",
    "\n",
    "$$\\boxed{P(h|D) = \\dfrac{P(D|h)\\cdot P(h)}{P(D)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCISE  \n",
    "\n",
    "The probability of having a specific type of cancer (C) in the population is 0.01. There is a screening test for this type of cancer. If a person has cancer, there is 10% chance that the test will come back negative. The test also has 5% chance of giving positive result for healthy people. \n",
    "\n",
    ">**Q1:** What is the probability of both having a positive test and having the cancer? \n",
    "\n",
    ">**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Q2:** If a patient gets a positive result from the test, what is the probability that s/he actually has the cancer? \n",
    "\n",
    ">**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Q3:** What is the probability that the patient is healthy?    \n",
    "\n",
    ">**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Q4:** The patient takes the test again and the result is still positive, what is the probability that the paitient has the cancer?\n",
    "\n",
    ">**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II | Bayes Theorem in NLP  \n",
    "\n",
    "Now you are familar with Bayes theorem, let's apply it to making predictions about words. In this case, Bayes theorem can be written as $$P(word|surroundingwords) = \\dfrac{P(surroundingwords|word)\\cdot P(word)} {P(surroundingwords)}$$\n",
    "\n",
    "Suppose we have the following quote:\n",
    "> \"So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\"\n",
    "\n",
    "Also suppose this text is the entire population of text we have to go from. We can ask a few questions based on this sentence:\n",
    "  \n",
    "  1. What is the probability that a randomly selected word from the sentence is \"you\"?\n",
    "  2. What is the probability that a randomly selected word from the sentence is \"if\"?\n",
    "  3. What is the probability of finding the word \"you\" after the word \"if\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "quote = \"So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the sentence to lower case\n",
    "quote = quote.lower()\n",
    "\n",
    "# remove punctuations in the sentence\n",
    "quote = quote.translate(None, string.punctuation)\n",
    "print quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the sentence\n",
    "words = quote.split()\n",
    "print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we use [the string method `split`](https://docs.python.org/2/library/stdtypes.html#string-methods) to split the sentences into words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: calculate the probability of word 'you'\n",
    "p_you = None\n",
    "print \"Probability of 'you' is %.4f\" % p_you\n",
    "\n",
    "# TODO: the probility of word 'if'\n",
    "p_if = None\n",
    "print \"Probability of 'if' is %.4f\" % p_if\n",
    "\n",
    "# TODO: the probility of finding 'you' after 'if'\n",
    "p_you_if = None\n",
    "print \"Probability of finding 'you' after 'if' is %.4f\" % p_you_if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Word\n",
    "\n",
    "We can calculate maximum likelihood word based on the preceding word. In this exercise, you will write a method, `NextWordProbability(sampletext, word)`, which takes in a string `sampletext` and a target word `word`, and creates a Python dictionary. The keys of the dictionary will be **the words that follow the target word `word`**, and the values will be the **number of times the key follows the target word `word`**. For example,  the output of the following code:\n",
    "```\n",
    "memo = \"If you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\"\n",
    "word = \"and\"\n",
    "print(NextWordProbability(memo,word))\n",
    "```\n",
    "should be the dictionary:\n",
    "```\n",
    "{'move': 1, 'pack': 1}\n",
    "```\n",
    "\n",
    "You may refer to [the Python documentation on dictionaries](https://docs.python.org/2/library/stdtypes.html#mapping-types-dict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: finish the implementation of NextWordProbability\n",
    "def NextWordProbability(sampletext, word):\n",
    "    \"\"\" \n",
    "    find the maximum likelihood word based on the preceding word \n",
    "    @param sampletext: a string\n",
    "    @param d: a word\n",
    "    \n",
    "    return dict: a dictionary\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    \n",
    "    \n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test cases:\n",
    "memo1 = \"If you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\"\n",
    "word1 = \"and\"\n",
    "print(NextWordProbability(memo1, word1))\n",
    "# Output should be: {'move': 1, 'pack': 1}\n",
    "\n",
    "memo2 = \"Milt, we're gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\"\n",
    "word2 = \"need\"\n",
    "print(NextWordProbability(memo2, word2))\n",
    "# Output should be: {'to': 1, 'all': 1}\n",
    "\n",
    "memo3 = \"Hello Peter, what's happening? Ummm, I'm gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I'm also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.\"\n",
    "word3 = \"in\"\n",
    "print(NextWordProbability(memo3, word3))\n",
    "# Output should be: {'tomorrow.': 1, 'on': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning Multiple Times\n",
    "\n",
    "Suppose now we have two missing words in a row: \"for --- ---\", and we would like to fill in the most likely candidate for the **second** missing word. Rahter than simply pick the maximum likelihood possibility for the first blank and using that to estiame the second, we can take the probabilities for all possibilities for the first blank. \n",
    "\n",
    "$$\\begin{array}{rcl}\n",
    "P(\\text{ \"for this\" }|\\text{\"for ---\"})&=&0.4\\\\\n",
    "P(\\text{ \"for that\" }|\\text{\"for ---\"})&=&0.3\\\\\n",
    "P(\\text{ \"for those\" }|\\text{\"for ---\"})&=&0.3\\end{array}$$\n",
    "\n",
    "$$\\begin{array}{rclrcl}\n",
    "P(\\text{ \"this time\" }|\\text{\"this ---\"})&=&0.6\\quad&P(\\text{ \"this job\" }|\\text{\"this ---\"})&=&0.4\\\\\n",
    "P(\\text{ \"that job\" }|\\text{\"that ---\"})&=&0.8\\quad&P(\\text{ \"that time\" }|\\text{\"that ---\"})&=&0.2\\\\\n",
    "P(\\text{ \"those items\" }|\\text{\"those ---\"})&=&1.0\\end{array}$$\n",
    "\n",
    "**Question:** Which word is the most likely candidate for the *second* missing word after \"for\"? ...with what probability?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Optimal Classifier\n",
    "\n",
    "We can compute the optimal label for a second missing word in a row based on the possible words that could be in the first blank. In this exercise, you will write a method `LaterWords(sampletext, word, distance)` which takes in a string `sampletext`, a target word `word`, and a given `distance`. It determines the most likely word to appear `distance` words after the target word `word`. For example, a call to the method:\n",
    "```\n",
    "LaterWords(memo, \"and\", 2)\n",
    "```\n",
    "would return a string: the most frequent word appearing 2 words after `\"and\"` in the string `memo`, *e.g.* \"and --- **---**\"\n",
    "\n",
    "Your task is to finish the implementation of `LaterWords` in the cell below. You may want to call your method `NextWordProbability()`, and you may refer to [the Python documentation on dictionaries](https://docs.python.org/2/library/stdtypes.html#mapping-types-dict). You can test your method on the test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: finish the implementation\n",
    "\n",
    "def LaterWords(sampletext, word, distance):\n",
    "    '''\n",
    "    @param sampletext: a sample of text to draw from\n",
    "    @param word: a word occuring before a corrupted sequence\n",
    "    @param distance: how many words later to estimate (i.e. 1 for the next word, 2 for the word after that)\n",
    "    @returns: a single word which is the most likely possibility\n",
    "    '''\n",
    "    \n",
    "    # TODO: Given a word, collect the relative probabilities of possible following words\n",
    "    # from @sampletext. You may want to import your code from the maximum likelihood exercise.\n",
    "    \n",
    "    # TODO: Repeat the above process--for each distance beyond 1, evaluate the words that\n",
    "    # might come after each word, and combine them weighting by relative probability\n",
    "    # into an estimate of what might appear next.\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test cases\n",
    "\n",
    "sample_memo = '''\n",
    "Milt, we're gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\n",
    "Oh, and remember: next Friday... is Hawaiian shirt day. So, you know, if you want to, go ahead and wear a Hawaiian shirt and jeans.\n",
    "Oh, oh, and I almost forgot. Ahh, I'm also gonna need you to go ahead and come in on Sunday, too...\n",
    "Hello Peter, whats happening? Ummm, I'm gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I'm also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.\n",
    "'''\n",
    "\n",
    "corrupted_memo = '''\n",
    "Yeah, I'm gonna --- you to go ahead --- --- complain about this. Oh, and if you could --- --- and sit at the kids' table, that'd be --- \n",
    "'''\n",
    "\n",
    "print(LaterWords(sample_memo, \"ahead\", 2))\n",
    "# Output: come\n",
    "print(LaterWords(sample_memo, \"and\", 3))\n",
    "# Output: on\n",
    "print(LaterWords(sample_memo, \"you\", 1))\n",
    "# Output: to"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
